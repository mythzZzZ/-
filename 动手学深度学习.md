## 2.预备知识





```python

# 整数
torch.arange(12)

x.shape

x.reshape(3, 4)

torch.zeros((2, 3, 4))  # 2x3x4  0矩阵

torch.ones((2, 3, 4))    # 2x3x4  1矩阵

torch.tensor([1.0, 2, 4, 8])

torch.randn(3, 4)  # 每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样  3x4矩阵

X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)          #dim0 0维度拼接   dim1  1维度拼接


X == Y    #匹配每一个位置
tensor([[False,  True, False,  True],
        [False, False, False, False],
        [False, False, False, False]])


X.sum()  #对x里的所有元素求和  返回 1x1

A.sum(axis=0)   #按列相加 一列中的每个元素加起来



A = X.numpy()
B = torch.tensor(A)      #tensor 与numpy转换


# 矩阵对应元素相乘
x = torch.tensor([1,2,3])
y = torch.tensor([1,2,3])
print(x * y )

#求两个向量的点积
torch.dot(x, y)

#矩阵向量积  A为矩阵 x为向量
torch.mv(A, x)

# 两个矩阵相乘
torch.mm(A, B)

    
    
    #更新梯度
    
x = torch.arange(4.0)
    
x.requires_grad_(True) 		#设置梯度的存储空间
    
    y = 2 * torch.dot(x, x)
    
    y.backward()   #y反向传播
    
   x.grad   #x更新梯度 
    
    
    x.grad.zero_()  #清除x的梯度
    
# 这个A更换了地址
A = A + B

# 这个A没有改变地址
A[:] = A + B
# 这个A没有改变地址
A += B

# 求tensor x中数据的个数
x.numel()

# 将张量变成一个标量
a.item()

# 通过分配新内存，将A的一个副本分配给B
B = A.clone()

# 对应位置元素相乘
A * B
```





## 3.线性回归





### 线性回归简单实现

```python
#线性回归简单实现

import torch
from d2l import torch as d2l
from torch.utils import data
import numpy as np
from torch import nn

true_w = torch.tensor([2, -3.4])
true_b = 4.2

#通过权重和偏置生成数据集
features, labels = d2l.synthetic_data(true_w, true_b, 1000) 

#把数据集加载到dataloader
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)

net = nn.Sequential(nn.Linear(2, 1))

#初始化网络一开始的偏置和权重
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

loss = nn.MSELoss()
# 定义优化器
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

num_epochs = 3
for epoch in range(num_epochs):
    #遍历数据加载器
    for X, y in data_iter:
        #计算损失
        l = loss(net(X) ,y)
        #反向传播之前 优化器清空梯度
        trainer.zero_grad()
        # 损失函数反向传播
        l.backward()
        #优化器更新梯度
        trainer.step()
    # 计算第一次迭代后 特征与标签的损失    
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')












```



训练必备东西

- 数据集
- 数据加载器
- 损失函数
- 优化器

训练过程

- 遍历数据与标签
- 计算损失
- 优化器清空原梯度
- 损失函数反向传播
- 优化器更新梯度





### softmax简单实现

- 为了避免softmax的计算值溢出 把softmax的计算结合到 loss里面

```python
import torch
from torch import nn
from d2l import torch as d2l


batch_size = 256
#加载数据集
train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)

#定义模型
net = nn.Sequential(nn.Flatten(),nn.Linear(784,10))

#定义初始化权重函数
def init_weights(m):
    if type(m) == nn.Linear:
         nn.init.normal_(m.weight, std=0.01)

#网络初始化权重
net.apply(init_weights)


loss = nn.CrossEntropyLoss(reduction='none')

trainer = torch.optim.SGD(net.parameters(), lr=0.1)

num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)


```



## 4.多层感知机



```python
x.detach() #把x张量的数据值提取出来，不保留x的梯度信息等等


# 清除以前的梯度
x.grad.data.zero_()
#y对张量like x反向传播  这个方向传播只反映方向  retain_graph=True 保留这个计算图 后面可以继续调用
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))

```



### 多层感知机的简单实现

```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(nn.Flatten(),
            nn.Linear(784, 256),
            nn.ReLU(),
            nn.Linear(256, 10))

#初始化权重函数
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)
        
#初始化权重
net.apply(init_weights);

batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction='none')
#定义优化器
trainer = torch.optim.SGD(net.parameters(), lr=lr)

#迭代数据
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

#训练模型
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)










```



### Dropout

```python

#  (torch.rand(X.shape) > 0.5) 生成概率大于0.5的 0，1矩阵 该矩阵内元素只有0或1
mask = (torch.rand(X.shape) > 0.5).float()  


import torch
from torch import nn
from d2l import torch as d2l

#定义dropout函数
def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # 在本情况中，所有元素都被丢弃
    if dropout == 1:
   		 return torch.zeros_like(X)
    # 在本情况中，所有元素都被保留
    if dropout == 0:
   		 return X
    mask = (torch.rand(X.shape) > dropout).float()
    return mask * X / (1.0 - dropout)



X= torch.arange(16, dtype = torch.float32).reshape((2, 8))
print(X)
print(dropout_layer(X, 0.))
print(dropout_layer(X, 0.5))
print(dropout_layer(X, 1.))
```



dropout训练

```python

net = nn.Sequential(nn.Flatten(),
                                nn.Linear(784, 256),
                                nn.ReLU(),
                                # 在第一个全连接层之后添加一个dropout层
                                nn.Dropout(dropout1),
                                nn.Linear(256, 256),
                                nn.ReLU(),
                                # 在第二个全连接层之后添加一个dropout层
                                nn.Dropout(dropout2),
                                nn.Linear(256, 10))


def init_weights(m):
    if type(m) == nn.Linear:
    	nn.init.normal_(m.weight, std=0.01)
net.apply(init_weights)


trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer

```





### 预测房价



```python
import hashlib
import os
import tarfile
import zipfile
import requests
import pandas as pd


test_data = pd.read_csv('./house/test.csv')

train_data = pd.read_csv('./house/train.csv')

train_data.drop('Id',axis = 1)


all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))  #从行的维度拼接数据  不要ID列

# 返回不是字符串类型数据的索引。 
# all_features.dtypes != 'object'：如果all_features每列数据类型不为 字符串(object)则返回index下标
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index


#处理不为字符串的数据和缺失的数据
#将所有缺失的值替换为相应特征的平均值
#所有数据 apply lambda表达式
all_features[numeric_features] = all_features[numeric_features].apply(
lambda x: (x - x.mean()) / (x.std()))

# 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0
all_features[numeric_features] = all_features[numeric_features].fillna(0)


# “Dummy_na=True”将“na”（缺失值）视为有效的特征值，并为其创建指示符特征
# 例如，“MSZoning”包含值“RL”和“Rm”。
# 我们将创建两个新的指示器特征“MSZoning_RL”和“MSZoning_RM”，其值为0或1。根据独热编码，如果
# “MSZoning”的原始值为“RL”，则：“MSZoning_RL”为1，“MSZoning_RM”为0
# dummy_na=True 创造新的数据列 来表示当独热编码数据列为空时的表示
all_features = pd.get_dummies(all_features, dummy_na=True)



# 获取训练数据的行数
n_train = train_data.shape[0]

import torch
import  torch.nn as nn

#从拼接数据中获取训练的数据 转换成tensor
train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)
#从拼接数据中获取测试的数据 转换成tensor
test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)


#获取标签数据 该数据为 SalePrice列的值 然后reshape(-1, 1) 转化为 n行1列 广播机制
train_labels = torch.tensor(
train_data.SalePrice.values.reshape(-1, 1), dtype=torch.float32)



loss = nn.MSELoss()

#获取列数
in_features = train_features.shape[1]

in_features

def get_net():
    net = nn.Sequential(nn.Linear(in_features,1))
    return net






# 输出均分根误差
def log_rmse(net, features, labels):
    # 为了在取对数时进一步稳定该值，将小于1的值设置为1
    #clamp函数限制 net的输出值  输出值小于1的值设置为1  最大值为float('inf') 无穷大
    clipped_preds = torch.clamp(net(features), 1, float('inf'))

# torch.log(clipped_preds)：这是对神经网络的预测值 clipped_preds 进行自然对数变换，
# 这种变换通常用于处理正数数据，可能是为了使数据更接近正态分布
# torch.sqrt 是 PyTorch 中的一个函数，用于计算输入张量中每个元素的平方根
    rmse = torch.sqrt(loss(torch.log(clipped_preds),torch.log(labels)))
    # item() 函数用于从 PyTorch 张量中提取标量值
    return rmse.item()


from d2l import torch as d2l

def train(net, train_features, train_labels, test_features, test_labels,
        num_epochs, learning_rate, weight_decay, batch_size):
    train_ls, test_ls = [], []

    #定义数据加载器
    train_iter = d2l.load_array((train_features, train_labels), batch_size)

    optimizer = torch.optim.Adam(net.parameters(),lr = learning_rate,weight_decay = weight_decay)

    for epoch in range(num_epochs):
        for X, y in train_iter:
            optimizer.zero_grad()
            l = loss(net(X), y)
            l.backward()
            optimizer.step()
        # 获取train集的均分根误差
        train_ls.append(log_rmse(net, train_features, train_labels))

        #获取test集的均分根误差
        if test_labels is not None:
            test_ls.append(log_rmse(net, test_features, test_labels))
    return train_ls, test_ls




# 使用所有数据进行训练
def train_and_pred(train_features, test_features, train_labels, test_data,
num_epochs, lr, weight_decay, batch_size):
    net = get_net()
    train_ls, _ = train(net, train_features, train_labels, None, None,
    num_epochs, lr, weight_decay, batch_size)
    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',
    ylabel='log rmse', xlim=[1, num_epochs], yscale='log')
    print(f'训练log rmse：{float(train_ls[-1]):f}')
    # 将网络应用于测试集。

    #将预测的结果转化为numpy格式
    preds = net(test_features).detach().numpy()
    
    
    
    
    # 将其重新格式化以导出到Kaggle
    # preds.reshape(1, -1)[0]将数据转化为一行 取出第一行
    # 将这一行结果赋值给 SalePrice列
    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])
    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)
    submission.to_csv('submission.csv', index=False)

train_and_pred(train_features, test_features, train_labels, test_data,
num_epochs, lr, weight_decay, batch_size)





```





## 5.深度学习计算





### 保存文件

```python
import torch
from torch import nn
from torch.nn import functional as F
x = torch.arange(4)

#保存张量 把x张量存到 x-file
torch.save(x, 'x-file')

#加载张量
x2 = torch.load('x-file')


y = torch.zeros(4)
#存储张量列表
torch.save([x, y],'x-files')
#读取张量列表
x2, y2 = torch.load('x-files')



#存储读取张量字典
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
mydict2



#存储  模型参数

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)
torch.save(net.state_dict(), 'mlpparams.pth')

#恢复模型参数
clone = MLP()
clone.load_state_dict(torch.load('mlpparams.pth'))

# 把神经网络切换到评估模式
clone.eval()




```



### 配置GPU

```python

torch.device('cpu')
torch.device('cuda')
torch.device('cuda:1')


#统计GPU个数
torch.cuda.device_count()


# 调用第i个GPU 如果不可用则调用CPU
def try_gpu(i=0):
    #@save
    """如果存在，则返回gpu(i)，否则返回cpu()"""
    if torch.cuda.device_count() >= i + 1:
   		 return torch.device(f'cuda:{i}')
    return torch.device('cpu')


# 调用多个GPU 返回GPU列表   如果不可用则调用CPU
def try_all_gpus():
    #@save
    """返回所有可用的GPU，如果没有GPU，则返回[cpu(),]"""
    devices = [torch.device(f'cuda:{i}')
    for i in range(torch.cuda.device_count())]
    		return devices if devices else [torch.device('cpu')]



# 把张量存储在GPU上
X = torch.ones(2, 3, device=try_gpu())

#  把张量存放在1gpu上
Y = torch.rand(2, 3, device=try_gpu(1))

               
#  把X张量复制到 GPU1上
Z = X.cuda(1)

#张量相加的时候要把张量复制到同一张GPU上 不然会错
X+Z               
               
               

        
# 把模型放到GPU上
#try_gpu() 是上面定义的函数

net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu())        
        
        
               

```





## 6.卷积神经网络



### 卷积示例

```python
# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核
conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)
# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），
# 其中批量大小和通道数都为1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2
# 学习率
for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    #  l.sum().backward() 通常用于计算整体损失的梯度，而 l.backward() 用于计算每个样本的梯度
    l.sum().backward()
    # 迭代卷积核
    # 这行代码的作用是使用梯度下降法来更新卷积层的权重。具体地说，它从权重张量中减去学习率乘以梯度的值，以便在训练过程中逐渐减小损失函数
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
 		   print(f'epoch {i+1}, loss {l.sum():.3f}')
            
            

```



### 池化

```python
pool2d = nn.MaxPool2d(3)    3x3的最大池化
```



### LeNet

```python
#@save
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
"""用GPU训练模型(在第六章定义)"""
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    print('training on', device)
    
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # 训练损失之和，训练准确率之和，样本数
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():
                # 不更新梯度计算
                # l * X.shape[0]  计算总的损失   d2l.accuracy(y_hat, y)计算准确率
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            #计算平均损失
            train_l = metric[0] / metric[2]
            #计算平均准确率
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
         test_acc = evaluate_accuracy_gpu(net, test_iter)
         animator.add(epoch + 1, (None, None, test_acc))
     print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
                      f'test acc {test_acc:.3f}')
      print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
                      f'on {str(device)}')
```





## 7.现代卷积神经网络



### VGG

```python
##VGG
import torch
from torch import nn
from d2l import torch as d2l

import os
os.chdir('/home/jupyter-zhangwenkk/data/')


def VGG_block(num_conv,in_channel,out_channel):
    layers = []
    for _ in range(num_conv):
        layers.append(nn.Conv2d(in_channel,out_channel,kernel_size=3,padding=1))
        layers.append(nn.ReLU())
        in_channel = out_channel
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))

    return nn.Sequential(*layers)


conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))

def vgg(con_arch):
    conv_blks = []
    in_channels = 1
    for (num_convs,out_channels) in con_arch:
        conv_blks.append(VGG_block(num_convs,in_channels,out_channels))
        in_channels = out_channels

    return  nn.Sequential(*conv_blks,nn.Flatten(),
                            # 全连接层部分
                            nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),
                            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
                            nn.Linear(4096, 10))


net = vgg(conv_arch)


X = torch.randn(size=(1, 1, 224, 224))
for blk in net:
    X = blk(X)
    print(blk.__class__.__name__,'output shape:\t',X.shape)


ratio = 4
small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]
net = vgg(small_conv_arch)


lr, num_epochs, batch_size = 0.05, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())



```



### BN





手写BN

```python
import torch
from torch import nn
from d2l import torch as d2l
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式
    if not torch.is_grad_enabled():
        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:	
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # 使用全连接层的情况，计算特征维上的均值和方差
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。
            # 这里我们需要保持X的形状以便后面可以做广播运算
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
            # 训练模式下，用当前的均值和方差做标准化
            X_hat = (X - mean) / torch.sqrt(var + eps)
            # 更新移动平均的均值和方差
         moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
         moving_var = momentum * moving_var + (1.0 - momentum) * var
      Y = gamma * X_hat + beta
            # 缩放和移位
      return Y, moving_mean.data, moving_var.dat
```











## 目标检测



RCNN

先提取图片的特征  输出坐标和特征，把特征送到SVM输出类别















## 要理解的概念(面试)

### 梯度下降

https://zhuanlan.zhihu.com/p/261375491

- 什么是梯度，方程求偏导得到的式子就是梯度，把对应的x值放进去就能得到x点的斜率，这样就可以知道要减少值时前进的方向
- 使用所有样本来更新梯度

### 随机梯度下降

SGD的核心思想是在每次迭代中随机选择一个样本（或一小批样本）来估计梯度，而不是使用整个数据集。这样做的优点是计算效率高，尤其是当数据集很大时。SGD也能够逃离局部最小值，因为随机性引入了一定的噪声，有助于模型探索更多的参数空间。

这个随机的梯度如何选择

- 使用一个样本来更新梯度

![image-20240519183139578](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240519183139578.png)

### 小批量随机梯度下降

https://zhuanlan.zhihu.com/p/72929546

- 使用一些样本来更新梯度

![image-20240529101550727](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240529101550727.png)



### 批量大小的选择

**批量大小不影响随机梯度的期望，但是会影响随机梯度的方差.** 批量大小越大，随机梯度的方差越小，引入的噪声也越小，训练也越稳定，因此可以设置较大的学习率. 而批量大小较小时，需要设置较小的学习率，否则模型会不收敛. 学习率通常要随着批量大小的增大而相应地增大. 一个简单有效的方法是线性缩放规则（Linear Scaling Rule）

https://cloud.baidu.com/article/3252849

![image-20240529102922029](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240529102922029.png)

为什么batchsize越大，随机误差越小（梯度的方差越小）？

- 在小批次（mini-batch）梯度下降法中，每次迭代都会计算一个批次样本的平均梯度。批次大小越大，这个平均梯度就越接近于整个训练集的真实梯度。   - 较大的批次包含更多的样本，这使得计算出的梯度能够更好地代表数据集的总体趋势，从而减少了随机误差





### 学习率衰减算法

学习率是神经网络优化时的重要超参数. 在梯度下降法中，学习率 𝛼 的取值非常关键，如果过大就不会收敛，如果过小则收敛速度太慢.常用的学习率调整方法包括**学习率衰减、学习率预热、周期性学习率调整**以及一些自适应调整学习率的方法

学习率一开始要大，保证收敛速度

后期学习率要变小，避免来回震荡

学习率衰减的常见方法

- 分段常数衰减
- 逆时衰减
- 指数衰减
- 自然指数衰减
- 余弦衰减
- 学习率衰减往往与衰减率，epoch，初始学习率有关



学习率预热

- 由于参数是随机初始化的，梯度往往也比较大，再加上比较大的初始学习率，会使得训练不稳定
- 为了提高训练稳定性，我们可以在最初几轮迭代时，采用比较小的学习率，等梯度下降到一定程度后再恢复到初始的学习率，这种方法称为学习率预热（Learning Rate Warmup）



周期性学习率调整

- 为了使得梯度下降法能够逃离局部最小值或鞍点，一种经验性的方式是在训练过程中周期性地增大学习率.
  虽然增大学习率可能短期内有损网络的收敛稳定性，但从长期来看有助于找到更好的局部最优解



循环学习率

- 让学习率在一个区间内周期性地增大和缩小



**自适应学习率算法**

AdaGrad

- 通过计算梯度平方的累计值来调整学习率

RMSprop 算法

- 计算每次迭代梯度 gt平方的指数衰减移动平均，

## 学习率调整策略

- linearLR 
  - 线性学习率
  - 预先定义起始学习率，结束学习率，迭代次数。从起始学习率开始经过定义的迭代次数达到结束学习率

![image-20240508145447541](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240508145447541.png)

- CosineAnnealingLR
  - 余弦退火学习率
  - 定义初始学习率，结束学习率，定义学习率周期。在一个后期内下降，然后一个周期内上升

![image-20240508145812784](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240508145812784.png)

- 组合两个学习率，达到学习率预热的目的

![image-20240508150240302](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240508150240302.png)



**动量算法**

Adam

- 计算梯度平方的指数加权平均，计算梯度 的指数加权平均

**梯度截断**

- 梯度的模小于或大于这个区间时就进行截断.避免梯度消失(爆炸)









### 参数初始化

在第一遍前向计算时，如果参数都为 0，所有的隐层神经元的激活值都相同. 这样会导致深层神经元没有区分性. 这种现象也称为**对称权重**现象.



我们要避免这个现象，对每个参数进行随机初始化

- 如果初始化参数太小，会导致网络梯度消失，如果过大会导致梯度爆炸（根据使用了哪一个激活函数来定义，不同的激活函数造成的后果可能不一样）



常用的初始化方法

- 高斯分布初始化
  - 参数从一个固定均值（比如 0）和固定方差（比如 0.01）的高斯分布进行随机初始化.
- 均匀分布初始化
  - 在一个给定的区间[-r,r]内采用均匀分布来初始化参数. 超参数 r 的设置可以按神经元的连接数量进行自适应的调整
- Xavier 初始化
  - 根据每层的神经元数量来自动计算初始化参数的方差.





### 数据预处理



- 标准归一化处理
- 白化处理
  - 用来降低输入数据特征之间的冗余性. 输入数据经过白化处理后，特征之间相关性较低，并且所有特征具有相同的方差.
  - 白化的一个主要实现方式是使用主成分分析（Principal Component Analy-sis，PCA）方法去除掉各个成分之间的相关性

![image-20240529150815252](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240529150815252.png)

### 超参数优化

- 网格搜索
  - 通过尝试所有超参数的组合来寻址合适一组超参数配置的方法
- 随机搜索
  - 一种在实践中比较有效的改进方法是对超参数进行随机组合，然后选取一个性能最好的配置



BN层的作用

![image-20241220110621380](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20241220110621380.png)









### softmax回归

![image-20240422102220406](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240422102220406.png)

**softmax要配合交叉熵损失函数同时实现**

- 这里需要注意一下，当使用Softmax函数作为输出节点的激活函数的时候，一般使用交叉熵作为损失函数。由于Softmax函数的数值计算过程中，很容易因为输出节点的输出值比较大而发生数值溢出的现象，在计算交叉熵的时候也可能会出现数值溢出的问题。







### 激活函数



sigmoid

- sigmoid通常称为挤压函数（squashing function）：它将范围（‐inf, inf）中的任意输入压缩到区间（0, 1）中的某个值
- $sigmoid(x) = \frac{1}{1+e^{-x}}$
- ![image-20240407152025211](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240407152025211.png)
- ![image-20240407170440187](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240407170440187.png)

sigmoid函数的缺点，取值被映射到0-1范围内，所以会出现梯度消失的问题

- ReLU
  - ReLU(x) = max(x, 0)
  - ReLU激活函数缓解了梯度消失问题，这样可以加速收敛

![image-20240407150503304](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240407150503304.png)

ReLU的好处

https://zhuanlan.zhihu.com/p/390655512

- 能够避免反向传播过程中的梯度消失、屏蔽负值、防止梯度饱和

ReLU的缺点

- 当参数取值为负数时，ReLU激活函数会输出0，

- ![image-20240521162531230](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240521162531230.png)

![image-20240528152751946](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240528152751946.png)









- LeaKyReLU
  - 在小于0的部分不像ReLU是0，而是一个负数的随机梯度，a的值一般为0.01

![image-20240422160856843](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240422160856843.png)







- tanh函数
  - $tanh(x) = \frac{1-e^{-2x}}{1+e^{-2x}}$ 
  - ![image-20240407154420481](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240407154420481.png)

![image-20240411152150367](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240411152150367.png)

**SiLu(swish)激活函数**

f(x) = x * sigmoid(x)  (yolov5)

它既有 ReLU（Rectified Linear Unit）激活函数的一些优点（例如，能够缓解梯度消失问题），又能解决 ReLU 函数的一些缺点（例如，ReLU 函数不是零中心的，且在负数部分的梯度为零）

![image-20240418183949125](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240418183949125.png)





### 激活函数的选择

如果搭建的神经网络层数不多，选择sigmoid、tanh、relu、softmax都可以；而如果搭建的网络层次较多，那就需要小心，**选择不当就可导致梯度消失问题**。**此时一般不宜选择sigmoid、tanh激活函数，因它们的导数都小于1**，所以，搭建比较深的神经网络时，一般使用relu激活函数





### 损失函数的选择

什么是损失函数？

损失函数（Loss Function）在机器学习中非常重要，因为训练模型的过程实际就是优化损失函数的过程。损失函数用来衡量模型的好坏，损失函数越小说明模型和参数越符合训练样本。



常用的损失函数有两种

- 交叉熵(Cross Entropy) softmax激活函数
- https://www.zhihu.com/tardis/zm/art/35709485?source_id=1005
  - 分类问题 (交叉熵反应的两个概率分布的距离)
    - 多分类
      - softmax激活函数+交叉熵损失函数
    - 二分类 （逻辑回归损失（Logistic Loss））

![image-20240501153923650](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240501153923650.png)

**为什么交叉熵可以计算类别的损失**

https://zhuanlan.zhihu.com/p/124309304

- **交叉熵（Cross Entropy）用于衡量一个概率分布与另一个概率分布之间的距离。交叉熵是机器学习和深度学习中常常用到的一个概念。在分类问题中，我们通常有一个真实的概率分布（通常是专家或训练数据的分布），以及一个模型生成的概率分布，交叉熵可以衡量这两个分布之间的距离。模型训练时，通过最小化交叉熵损失函数，我们可以使模型预测值的概率分布逐步接近真实的概率分布。**

- 神经网络的类别输出往往是输出类别的概率值，这时候要计算输出概率值与标签概率之间的误差。这个要追溯到信息论，香浓熵。当标签的概率分布为P，模型预测的概率为Q，我们要减小P，Q之间的误差，就要对PQ的交叉熵做最小化处理，这时候就要最小化PQ的交叉熵，让他们的概率分布更接近

![image-20240505212319942](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240505212319942.png)

















- 均方误差（Mean squared error，MSE)
- https://blog.csdn.net/Xiaobai_rabbit0/article/details/111032136
  - 回归问题（回归问题预测的不是类别，而是一个任意实数）



![image-20240501154023940](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240501154023940.png)

### 优化器



![image-20240411153814708](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240411153814708.png)



- 传统梯度下降



- 动量算法 + 梯度下降
  - NAG![image-20240411154623342](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240411154623342.png)

- 学习率算法

  - AdaGrad（自动更新学习率）
    - 为了更好地驾驭这个超参数，人们想出来多种自适应优化算法，使用自适应优化算法，学习率不
      再是一个固定不变值，它会根据不同情况自动调整来适应相应的情况

  - RMSProp算法
    - 针对梯度平方和累计越来越大的问题，RMSProp指数加权的移动平均代替梯度平方和
  - Adam
    - Adam（Adaptive Moment Estimation）本质上是带有动量项的RMSprop，它利用梯度
      的一阶矩估计和二阶矩估计动态调整每个参数的学习率
    - Adam 通过计算梯度的一阶矩估计和二阶矩估计的移动平均值，调整每个参数的学习率



算法的综合使用

- **有时可以考虑综合使用这些优化算法，如采用先使用Adam，然后使用SGD的优化方**
  **法，这个想法，实际上是由于在训练的早期阶段SGD对参数调整和初始化非常敏感。因**
  **此，我们可以通过先使用Adam优化算法来进行训练，这将大大地节省训练时间，且不必**
  **担心初始化和参数调整，一旦用Adam训练获得较好的参数后，就可以切换到SGD+动量**
  **优化，以达到最佳性能**





adamW

- 优化器是用来优化梯度的，通过距离范数来优化下一次下降的梯度值

![image-20240508151557549](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240508151557549.png)

















欠拟合（模型太简单，无法很好学习到数据集中的特征，）

过拟合（模型太复杂，学习数据集中的特征学过头了，数据集太简单）

![image-20240407160037091](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240407160037091.png)

- 数据集太小容易过拟合
- 数据集太大容易欠拟合(模型无法很好学习数据集中的规律)











### 解决过拟合问题

- 权重衰减
  - 最常用方法是将权重向量的范数作为惩罚项加到最小化损失的问题中。**权重衰减通过对模型的权重参数进行惩罚，使得模型倾向于学习简单的权重分布，而不是复杂的分布，从而让模型变得简单。**
- dropout（暂退法）（丢弃法）
  - f(x) = wx + b  当w=0时，相当于这个神经元不作为输入了，训练过程中丢弃一些神经元，所以模型有变简单了

- 提前停止
  - 当验证集上的错误率不再下降，就停止迭代

### 梯度消失

- 参数更新过小，在每次更新时权重几乎不会移动，导致模型无法学习
- 用了不合适的激活函数

#### 梯度爆炸

- 参数更新过大，破坏了模型的稳定收敛；
- 梯度的初始值太大，降低初始值，设置梯度阈值，强制在这个范围内
- 通过权重正则项惩罚来限制权重大小
- batch normalization **对每一层的输出规范为均值和方差一致的方法**，消除权重参数放大缩小带来的影响，从而解决梯度消失和梯度爆炸（BN将输出从饱和区拉到非饱和区）



### 模型轻量化

- DW卷积的使用

- 点卷积代替全连接层



### 卷积的特性

- 平移不变性
  - 卷积过程中卷积核是平移的，但是输入特征的位置对应着输出特征的位置
- 局部性
  - 卷积核 探索输入图像中的局部区域，不在意图像中相隔较远的区域关系



输出HW的计算公式

![image-20240410094800541](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240410094800541.png)

汇聚层

- 池化层



### 批量归一化(batch normalization)

让数据变成均值为0，方差为1的分布，让数据变成独立同分布

这样做有什么好处

- 让模型更好的训练
  - 让特征的数值映射到一个区间分布。如果一个特征的分布趋向于两边，使用sigmoid激活函数激活时，可能要么是0 要么是1
- 减少训练和测试的误差 （让训练和测试时，数据的分布一样）
  - 训练数据与测试数据的分布是相同的，这减少了模型的偏差风险。这样，模型在训练数据上的表现和在实际应用中的表现更加一致。





### 凸函数与非凸优化

![image-20240410161028636](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240410161028636.png)



### 特征表示形式

局部表示

分布式表示





### 分析为什么平方损失函数不适用于分类问题

平方损失函数是计算两个连续值之间的距离，而分类问题输出的是一个离散值（概率），标签也是一个离散值，没有连续的概念。如果使用平方损失函数计算预测值与标签之间的距离没有实际意义，预测值和标签两个向量之间的平方差这个值不能反应分类这个问题的优化程度。



### 多层感知机（MLP）

多层感知机MLP层往往通过Linear降为 -》激活函数-》Linear升维

为什么要降维

- 在原始的高维空间中，包含冗余信息和噪声信息，会在实际应用中引入误差，影响准确率；
- 而降维可以提取数据内部的本质结构，减少冗余信息和噪声信息造成的误差，提高应用中的精度。

![image-20240521170640598](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240521170640598.png)



为什么要激活函数

- 不使用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。
- 使用激活函数，能够给神经元引入非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以利用到更多的非线性模型中。













### 常用库

- torchvision.transforms
- torchvision.datasets
- torch.utils.data.DataLoader
- torch.nn as nn (特征提取模块 损失函数)
- torch.nn.functional as F （激活函数）
- torch.optim as optim

```python
import torch
import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
download=False, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
download=False, transform=transform)

testloader = torch.utils.data.DataLoader(testset, batch_size=4,
shuffle=False, num_workers=2)

dataiter = iter(trainloader)
images, labels = dataiter.next()



import torch.nn as nn
import torch.nn.functional as F
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
class CNNNet(nn.Module):
    def __init__(self):
        super(CNNNet,self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=1)
        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)
        self.conv2 = nn.Conv2d(in_channels=16,out_channels=36,kernel_size=3,stride=1)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(1296,128)
        self.fc2 = nn.Linear(128,10)
    def forward(self,x):
        x=self.pool1(F.relu(self.conv1(x)))
        x=self.pool2(F.relu(self.conv2(x)))
         #print(x.shape)
        x=x.view(-1,36*6*6)
        x=F.relu(self.fc2(F.relu(self.fc1(x))))
        return x
net = CNNNet()
net=net.to(device)

import torch.optim as optim
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)


# 训练模型
for epoch in range(10):
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # 获取训练数据
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        # 权重参数梯度清零
        optimizer.zero_grad()
        # 正向及反向传播
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        # 显示损失值
        running_loss += loss.item()
        if i % 2000 == 1999: # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0
            print('Finished Training')
            
            
# 测试模型
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        images, labels = images.to(device), labels.to(device)
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        print('Accuracy of the network on the 10000 test images: %d %%' % (
            100 * correct / total))

```

