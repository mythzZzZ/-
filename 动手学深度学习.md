## 2.é¢„å¤‡çŸ¥è¯†





```python

# æ•´æ•°
torch.arange(12)

x.shape

x.reshape(3, 4)

torch.zeros((2, 3, 4))  # 2x3x4  0çŸ©é˜µ

torch.ones((2, 3, 4))    # 2x3x4  1çŸ©é˜µ

torch.tensor([1.0, 2, 4, 8])

torch.randn(3, 4)  # æ¯ä¸ªå…ƒç´ éƒ½ä»å‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º1çš„æ ‡å‡†é«˜æ–¯åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ä¸­éšæœºé‡‡æ ·  3x4çŸ©é˜µ

X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)          #dim0 0ç»´åº¦æ‹¼æ¥   dim1  1ç»´åº¦æ‹¼æ¥


X == Y    #åŒ¹é…æ¯ä¸€ä¸ªä½ç½®
tensor([[False,  True, False,  True],
        [False, False, False, False],
        [False, False, False, False]])


X.sum()  #å¯¹xé‡Œçš„æ‰€æœ‰å…ƒç´ æ±‚å’Œ  è¿”å› 1x1

A.sum(axis=0)   #æŒ‰åˆ—ç›¸åŠ  ä¸€åˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ åŠ èµ·æ¥



A = X.numpy()
B = torch.tensor(A)      #tensor ä¸numpyè½¬æ¢


# çŸ©é˜µå¯¹åº”å…ƒç´ ç›¸ä¹˜
x = torch.tensor([1,2,3])
y = torch.tensor([1,2,3])
print(x * y )

#æ±‚ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯
torch.dot(x, y)

#çŸ©é˜µå‘é‡ç§¯  Aä¸ºçŸ©é˜µ xä¸ºå‘é‡
torch.mv(A, x)

# ä¸¤ä¸ªçŸ©é˜µç›¸ä¹˜
torch.mm(A, B)

    
    
    #æ›´æ–°æ¢¯åº¦
    
x = torch.arange(4.0)
    
x.requires_grad_(True) 		#è®¾ç½®æ¢¯åº¦çš„å­˜å‚¨ç©ºé—´
    
    y = 2 * torch.dot(x, x)
    
    y.backward()   #yåå‘ä¼ æ’­
    
   x.grad   #xæ›´æ–°æ¢¯åº¦ 
    
    
    x.grad.zero_()  #æ¸…é™¤xçš„æ¢¯åº¦
    
# è¿™ä¸ªAæ›´æ¢äº†åœ°å€
A = A + B

# è¿™ä¸ªAæ²¡æœ‰æ”¹å˜åœ°å€
A[:] = A + B
# è¿™ä¸ªAæ²¡æœ‰æ”¹å˜åœ°å€
A += B

# æ±‚tensor xä¸­æ•°æ®çš„ä¸ªæ•°
x.numel()

# å°†å¼ é‡å˜æˆä¸€ä¸ªæ ‡é‡
a.item()

# é€šè¿‡åˆ†é…æ–°å†…å­˜ï¼Œå°†Açš„ä¸€ä¸ªå‰¯æœ¬åˆ†é…ç»™B
B = A.clone()

# å¯¹åº”ä½ç½®å…ƒç´ ç›¸ä¹˜
A * B
```





## 3.çº¿æ€§å›å½’





### çº¿æ€§å›å½’ç®€å•å®ç°

```python
#çº¿æ€§å›å½’ç®€å•å®ç°

import torch
from d2l import torch as d2l
from torch.utils import data
import numpy as np
from torch import nn

true_w = torch.tensor([2, -3.4])
true_b = 4.2

#é€šè¿‡æƒé‡å’Œåç½®ç”Ÿæˆæ•°æ®é›†
features, labels = d2l.synthetic_data(true_w, true_b, 1000) 

#æŠŠæ•°æ®é›†åŠ è½½åˆ°dataloader
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """æ„é€ ä¸€ä¸ªPyTorchæ•°æ®è¿­ä»£å™¨"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)

net = nn.Sequential(nn.Linear(2, 1))

#åˆå§‹åŒ–ç½‘ç»œä¸€å¼€å§‹çš„åç½®å’Œæƒé‡
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

loss = nn.MSELoss()
# å®šä¹‰ä¼˜åŒ–å™¨
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

num_epochs = 3
for epoch in range(num_epochs):
    #éå†æ•°æ®åŠ è½½å™¨
    for X, y in data_iter:
        #è®¡ç®—æŸå¤±
        l = loss(net(X) ,y)
        #åå‘ä¼ æ’­ä¹‹å‰ ä¼˜åŒ–å™¨æ¸…ç©ºæ¢¯åº¦
        trainer.zero_grad()
        # æŸå¤±å‡½æ•°åå‘ä¼ æ’­
        l.backward()
        #ä¼˜åŒ–å™¨æ›´æ–°æ¢¯åº¦
        trainer.step()
    # è®¡ç®—ç¬¬ä¸€æ¬¡è¿­ä»£å ç‰¹å¾ä¸æ ‡ç­¾çš„æŸå¤±    
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')












```



è®­ç»ƒå¿…å¤‡ä¸œè¥¿

- æ•°æ®é›†
- æ•°æ®åŠ è½½å™¨
- æŸå¤±å‡½æ•°
- ä¼˜åŒ–å™¨

è®­ç»ƒè¿‡ç¨‹

- éå†æ•°æ®ä¸æ ‡ç­¾
- è®¡ç®—æŸå¤±
- ä¼˜åŒ–å™¨æ¸…ç©ºåŸæ¢¯åº¦
- æŸå¤±å‡½æ•°åå‘ä¼ æ’­
- ä¼˜åŒ–å™¨æ›´æ–°æ¢¯åº¦





### softmaxç®€å•å®ç°

- ä¸ºäº†é¿å…softmaxçš„è®¡ç®—å€¼æº¢å‡º æŠŠsoftmaxçš„è®¡ç®—ç»“åˆåˆ° lossé‡Œé¢

```python
import torch
from torch import nn
from d2l import torch as d2l


batch_size = 256
#åŠ è½½æ•°æ®é›†
train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)

#å®šä¹‰æ¨¡å‹
net = nn.Sequential(nn.Flatten(),nn.Linear(784,10))

#å®šä¹‰åˆå§‹åŒ–æƒé‡å‡½æ•°
def init_weights(m):
    if type(m) == nn.Linear:
         nn.init.normal_(m.weight, std=0.01)

#ç½‘ç»œåˆå§‹åŒ–æƒé‡
net.apply(init_weights)


loss = nn.CrossEntropyLoss(reduction='none')

trainer = torch.optim.SGD(net.parameters(), lr=0.1)

num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)


```



## 4.å¤šå±‚æ„ŸçŸ¥æœº



```python
x.detach() #æŠŠxå¼ é‡çš„æ•°æ®å€¼æå–å‡ºæ¥ï¼Œä¸ä¿ç•™xçš„æ¢¯åº¦ä¿¡æ¯ç­‰ç­‰


# æ¸…é™¤ä»¥å‰çš„æ¢¯åº¦
x.grad.data.zero_()
#yå¯¹å¼ é‡like xåå‘ä¼ æ’­  è¿™ä¸ªæ–¹å‘ä¼ æ’­åªåæ˜ æ–¹å‘  retain_graph=True ä¿ç•™è¿™ä¸ªè®¡ç®—å›¾ åé¢å¯ä»¥ç»§ç»­è°ƒç”¨
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))

```



### å¤šå±‚æ„ŸçŸ¥æœºçš„ç®€å•å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(nn.Flatten(),
            nn.Linear(784, 256),
            nn.ReLU(),
            nn.Linear(256, 10))

#åˆå§‹åŒ–æƒé‡å‡½æ•°
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)
        
#åˆå§‹åŒ–æƒé‡
net.apply(init_weights);

batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction='none')
#å®šä¹‰ä¼˜åŒ–å™¨
trainer = torch.optim.SGD(net.parameters(), lr=lr)

#è¿­ä»£æ•°æ®
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

#è®­ç»ƒæ¨¡å‹
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)










```



### Dropout

```python

#  (torch.rand(X.shape) > 0.5) ç”Ÿæˆæ¦‚ç‡å¤§äº0.5çš„ 0ï¼Œ1çŸ©é˜µ è¯¥çŸ©é˜µå†…å…ƒç´ åªæœ‰0æˆ–1
mask = (torch.rand(X.shape) > 0.5).float()  


import torch
from torch import nn
from d2l import torch as d2l

#å®šä¹‰dropoutå‡½æ•°
def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # åœ¨æœ¬æƒ…å†µä¸­ï¼Œæ‰€æœ‰å…ƒç´ éƒ½è¢«ä¸¢å¼ƒ
    if dropout == 1:
   		 return torch.zeros_like(X)
    # åœ¨æœ¬æƒ…å†µä¸­ï¼Œæ‰€æœ‰å…ƒç´ éƒ½è¢«ä¿ç•™
    if dropout == 0:
   		 return X
    mask = (torch.rand(X.shape) > dropout).float()
    return mask * X / (1.0 - dropout)



X= torch.arange(16, dtype = torch.float32).reshape((2, 8))
print(X)
print(dropout_layer(X, 0.))
print(dropout_layer(X, 0.5))
print(dropout_layer(X, 1.))
```



dropoutè®­ç»ƒ

```python

net = nn.Sequential(nn.Flatten(),
                                nn.Linear(784, 256),
                                nn.ReLU(),
                                # åœ¨ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ ä¸€ä¸ªdropoutå±‚
                                nn.Dropout(dropout1),
                                nn.Linear(256, 256),
                                nn.ReLU(),
                                # åœ¨ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ ä¸€ä¸ªdropoutå±‚
                                nn.Dropout(dropout2),
                                nn.Linear(256, 10))


def init_weights(m):
    if type(m) == nn.Linear:
    	nn.init.normal_(m.weight, std=0.01)
net.apply(init_weights)


trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer

```





### é¢„æµ‹æˆ¿ä»·



```python
import hashlib
import os
import tarfile
import zipfile
import requests
import pandas as pd


test_data = pd.read_csv('./house/test.csv')

train_data = pd.read_csv('./house/train.csv')

train_data.drop('Id',axis = 1)


all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))  #ä»è¡Œçš„ç»´åº¦æ‹¼æ¥æ•°æ®  ä¸è¦IDåˆ—

# è¿”å›ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹æ•°æ®çš„ç´¢å¼•ã€‚ 
# all_features.dtypes != 'object'ï¼šå¦‚æœall_featuresæ¯åˆ—æ•°æ®ç±»å‹ä¸ä¸º å­—ç¬¦ä¸²(object)åˆ™è¿”å›indexä¸‹æ ‡
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index


#å¤„ç†ä¸ä¸ºå­—ç¬¦ä¸²çš„æ•°æ®å’Œç¼ºå¤±çš„æ•°æ®
#å°†æ‰€æœ‰ç¼ºå¤±çš„å€¼æ›¿æ¢ä¸ºç›¸åº”ç‰¹å¾çš„å¹³å‡å€¼
#æ‰€æœ‰æ•°æ® apply lambdaè¡¨è¾¾å¼
all_features[numeric_features] = all_features[numeric_features].apply(
lambda x: (x - x.mean()) / (x.std()))

# åœ¨æ ‡å‡†åŒ–æ•°æ®ä¹‹åï¼Œæ‰€æœ‰å‡å€¼æ¶ˆå¤±ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†ç¼ºå¤±å€¼è®¾ç½®ä¸º0
all_features[numeric_features] = all_features[numeric_features].fillna(0)


# â€œDummy_na=Trueâ€å°†â€œnaâ€ï¼ˆç¼ºå¤±å€¼ï¼‰è§†ä¸ºæœ‰æ•ˆçš„ç‰¹å¾å€¼ï¼Œå¹¶ä¸ºå…¶åˆ›å»ºæŒ‡ç¤ºç¬¦ç‰¹å¾
# ä¾‹å¦‚ï¼Œâ€œMSZoningâ€åŒ…å«å€¼â€œRLâ€å’Œâ€œRmâ€ã€‚
# æˆ‘ä»¬å°†åˆ›å»ºä¸¤ä¸ªæ–°çš„æŒ‡ç¤ºå™¨ç‰¹å¾â€œMSZoning_RLâ€å’Œâ€œMSZoning_RMâ€ï¼Œå…¶å€¼ä¸º0æˆ–1ã€‚æ ¹æ®ç‹¬çƒ­ç¼–ç ï¼Œå¦‚æœ
# â€œMSZoningâ€çš„åŸå§‹å€¼ä¸ºâ€œRLâ€ï¼Œåˆ™ï¼šâ€œMSZoning_RLâ€ä¸º1ï¼Œâ€œMSZoning_RMâ€ä¸º0
# dummy_na=True åˆ›é€ æ–°çš„æ•°æ®åˆ— æ¥è¡¨ç¤ºå½“ç‹¬çƒ­ç¼–ç æ•°æ®åˆ—ä¸ºç©ºæ—¶çš„è¡¨ç¤º
all_features = pd.get_dummies(all_features, dummy_na=True)



# è·å–è®­ç»ƒæ•°æ®çš„è¡Œæ•°
n_train = train_data.shape[0]

import torch
import  torch.nn as nn

#ä»æ‹¼æ¥æ•°æ®ä¸­è·å–è®­ç»ƒçš„æ•°æ® è½¬æ¢æˆtensor
train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)
#ä»æ‹¼æ¥æ•°æ®ä¸­è·å–æµ‹è¯•çš„æ•°æ® è½¬æ¢æˆtensor
test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)


#è·å–æ ‡ç­¾æ•°æ® è¯¥æ•°æ®ä¸º SalePriceåˆ—çš„å€¼ ç„¶åreshape(-1, 1) è½¬åŒ–ä¸º nè¡Œ1åˆ— å¹¿æ’­æœºåˆ¶
train_labels = torch.tensor(
train_data.SalePrice.values.reshape(-1, 1), dtype=torch.float32)



loss = nn.MSELoss()

#è·å–åˆ—æ•°
in_features = train_features.shape[1]

in_features

def get_net():
    net = nn.Sequential(nn.Linear(in_features,1))
    return net






# è¾“å‡ºå‡åˆ†æ ¹è¯¯å·®
def log_rmse(net, features, labels):
    # ä¸ºäº†åœ¨å–å¯¹æ•°æ—¶è¿›ä¸€æ­¥ç¨³å®šè¯¥å€¼ï¼Œå°†å°äº1çš„å€¼è®¾ç½®ä¸º1
    #clampå‡½æ•°é™åˆ¶ netçš„è¾“å‡ºå€¼  è¾“å‡ºå€¼å°äº1çš„å€¼è®¾ç½®ä¸º1  æœ€å¤§å€¼ä¸ºfloat('inf') æ— ç©·å¤§
    clipped_preds = torch.clamp(net(features), 1, float('inf'))

# torch.log(clipped_preds)ï¼šè¿™æ˜¯å¯¹ç¥ç»ç½‘ç»œçš„é¢„æµ‹å€¼ clipped_preds è¿›è¡Œè‡ªç„¶å¯¹æ•°å˜æ¢ï¼Œ
# è¿™ç§å˜æ¢é€šå¸¸ç”¨äºå¤„ç†æ­£æ•°æ•°æ®ï¼Œå¯èƒ½æ˜¯ä¸ºäº†ä½¿æ•°æ®æ›´æ¥è¿‘æ­£æ€åˆ†å¸ƒ
# torch.sqrt æ˜¯ PyTorch ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè®¡ç®—è¾“å…¥å¼ é‡ä¸­æ¯ä¸ªå…ƒç´ çš„å¹³æ–¹æ ¹
    rmse = torch.sqrt(loss(torch.log(clipped_preds),torch.log(labels)))
    # item() å‡½æ•°ç”¨äºä» PyTorch å¼ é‡ä¸­æå–æ ‡é‡å€¼
    return rmse.item()


from d2l import torch as d2l

def train(net, train_features, train_labels, test_features, test_labels,
        num_epochs, learning_rate, weight_decay, batch_size):
    train_ls, test_ls = [], []

    #å®šä¹‰æ•°æ®åŠ è½½å™¨
    train_iter = d2l.load_array((train_features, train_labels), batch_size)

    optimizer = torch.optim.Adam(net.parameters(),lr = learning_rate,weight_decay = weight_decay)

    for epoch in range(num_epochs):
        for X, y in train_iter:
            optimizer.zero_grad()
            l = loss(net(X), y)
            l.backward()
            optimizer.step()
        # è·å–trainé›†çš„å‡åˆ†æ ¹è¯¯å·®
        train_ls.append(log_rmse(net, train_features, train_labels))

        #è·å–testé›†çš„å‡åˆ†æ ¹è¯¯å·®
        if test_labels is not None:
            test_ls.append(log_rmse(net, test_features, test_labels))
    return train_ls, test_ls




# ä½¿ç”¨æ‰€æœ‰æ•°æ®è¿›è¡Œè®­ç»ƒ
def train_and_pred(train_features, test_features, train_labels, test_data,
num_epochs, lr, weight_decay, batch_size):
    net = get_net()
    train_ls, _ = train(net, train_features, train_labels, None, None,
    num_epochs, lr, weight_decay, batch_size)
    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',
    ylabel='log rmse', xlim=[1, num_epochs], yscale='log')
    print(f'è®­ç»ƒlog rmseï¼š{float(train_ls[-1]):f}')
    # å°†ç½‘ç»œåº”ç”¨äºæµ‹è¯•é›†ã€‚

    #å°†é¢„æµ‹çš„ç»“æœè½¬åŒ–ä¸ºnumpyæ ¼å¼
    preds = net(test_features).detach().numpy()
    
    
    
    
    # å°†å…¶é‡æ–°æ ¼å¼åŒ–ä»¥å¯¼å‡ºåˆ°Kaggle
    # preds.reshape(1, -1)[0]å°†æ•°æ®è½¬åŒ–ä¸ºä¸€è¡Œ å–å‡ºç¬¬ä¸€è¡Œ
    # å°†è¿™ä¸€è¡Œç»“æœèµ‹å€¼ç»™ SalePriceåˆ—
    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])
    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)
    submission.to_csv('submission.csv', index=False)

train_and_pred(train_features, test_features, train_labels, test_data,
num_epochs, lr, weight_decay, batch_size)





```





## 5.æ·±åº¦å­¦ä¹ è®¡ç®—





### ä¿å­˜æ–‡ä»¶

```python
import torch
from torch import nn
from torch.nn import functional as F
x = torch.arange(4)

#ä¿å­˜å¼ é‡ æŠŠxå¼ é‡å­˜åˆ° x-file
torch.save(x, 'x-file')

#åŠ è½½å¼ é‡
x2 = torch.load('x-file')


y = torch.zeros(4)
#å­˜å‚¨å¼ é‡åˆ—è¡¨
torch.save([x, y],'x-files')
#è¯»å–å¼ é‡åˆ—è¡¨
x2, y2 = torch.load('x-files')



#å­˜å‚¨è¯»å–å¼ é‡å­—å…¸
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
mydict2



#å­˜å‚¨  æ¨¡å‹å‚æ•°

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)
torch.save(net.state_dict(), 'mlpparams.pth')

#æ¢å¤æ¨¡å‹å‚æ•°
clone = MLP()
clone.load_state_dict(torch.load('mlpparams.pth'))

# æŠŠç¥ç»ç½‘ç»œåˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
clone.eval()




```



### é…ç½®GPU

```python

torch.device('cpu')
torch.device('cuda')
torch.device('cuda:1')


#ç»Ÿè®¡GPUä¸ªæ•°
torch.cuda.device_count()


# è°ƒç”¨ç¬¬iä¸ªGPU å¦‚æœä¸å¯ç”¨åˆ™è°ƒç”¨CPU
def try_gpu(i=0):
    #@save
    """å¦‚æœå­˜åœ¨ï¼Œåˆ™è¿”å›gpu(i)ï¼Œå¦åˆ™è¿”å›cpu()"""
    if torch.cuda.device_count() >= i + 1:
   		 return torch.device(f'cuda:{i}')
    return torch.device('cpu')


# è°ƒç”¨å¤šä¸ªGPU è¿”å›GPUåˆ—è¡¨   å¦‚æœä¸å¯ç”¨åˆ™è°ƒç”¨CPU
def try_all_gpus():
    #@save
    """è¿”å›æ‰€æœ‰å¯ç”¨çš„GPUï¼Œå¦‚æœæ²¡æœ‰GPUï¼Œåˆ™è¿”å›[cpu(),]"""
    devices = [torch.device(f'cuda:{i}')
    for i in range(torch.cuda.device_count())]
    		return devices if devices else [torch.device('cpu')]



# æŠŠå¼ é‡å­˜å‚¨åœ¨GPUä¸Š
X = torch.ones(2, 3, device=try_gpu())

#  æŠŠå¼ é‡å­˜æ”¾åœ¨1gpuä¸Š
Y = torch.rand(2, 3, device=try_gpu(1))

               
#  æŠŠXå¼ é‡å¤åˆ¶åˆ° GPU1ä¸Š
Z = X.cuda(1)

#å¼ é‡ç›¸åŠ çš„æ—¶å€™è¦æŠŠå¼ é‡å¤åˆ¶åˆ°åŒä¸€å¼ GPUä¸Š ä¸ç„¶ä¼šé”™
X+Z               
               
               

        
# æŠŠæ¨¡å‹æ”¾åˆ°GPUä¸Š
#try_gpu() æ˜¯ä¸Šé¢å®šä¹‰çš„å‡½æ•°

net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu())        
        
        
               

```





## 6.å·ç§¯ç¥ç»ç½‘ç»œ



### å·ç§¯ç¤ºä¾‹

```python
# æ„é€ ä¸€ä¸ªäºŒç»´å·ç§¯å±‚ï¼Œå®ƒå…·æœ‰1ä¸ªè¾“å‡ºé€šé“å’Œå½¢çŠ¶ä¸ºï¼ˆ1ï¼Œ2ï¼‰çš„å·ç§¯æ ¸
conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)
# è¿™ä¸ªäºŒç»´å·ç§¯å±‚ä½¿ç”¨å››ç»´è¾“å…¥å’Œè¾“å‡ºæ ¼å¼ï¼ˆæ‰¹é‡å¤§å°ã€é€šé“ã€é«˜åº¦ã€å®½åº¦ï¼‰ï¼Œ
# å…¶ä¸­æ‰¹é‡å¤§å°å’Œé€šé“æ•°éƒ½ä¸º1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2
# å­¦ä¹ ç‡
for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    #  l.sum().backward() é€šå¸¸ç”¨äºè®¡ç®—æ•´ä½“æŸå¤±çš„æ¢¯åº¦ï¼Œè€Œ l.backward() ç”¨äºè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦
    l.sum().backward()
    # è¿­ä»£å·ç§¯æ ¸
    # è¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥æ›´æ–°å·ç§¯å±‚çš„æƒé‡ã€‚å…·ä½“åœ°è¯´ï¼Œå®ƒä»æƒé‡å¼ é‡ä¸­å‡å»å­¦ä¹ ç‡ä¹˜ä»¥æ¢¯åº¦çš„å€¼ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å‡å°æŸå¤±å‡½æ•°
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
 		   print(f'epoch {i+1}, loss {l.sum():.3f}')
            
            

```



### æ± åŒ–

```python
pool2d = nn.MaxPool2d(3)    3x3çš„æœ€å¤§æ± åŒ–
```



### LeNet

```python
#@save
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
"""ç”¨GPUè®­ç»ƒæ¨¡å‹(åœ¨ç¬¬å…­ç« å®šä¹‰)"""
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    print('training on', device)
    
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # è®­ç»ƒæŸå¤±ä¹‹å’Œï¼Œè®­ç»ƒå‡†ç¡®ç‡ä¹‹å’Œï¼Œæ ·æœ¬æ•°
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():
                # ä¸æ›´æ–°æ¢¯åº¦è®¡ç®—
                # l * X.shape[0]  è®¡ç®—æ€»çš„æŸå¤±   d2l.accuracy(y_hat, y)è®¡ç®—å‡†ç¡®ç‡
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            #è®¡ç®—å¹³å‡æŸå¤±
            train_l = metric[0] / metric[2]
            #è®¡ç®—å¹³å‡å‡†ç¡®ç‡
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
         test_acc = evaluate_accuracy_gpu(net, test_iter)
         animator.add(epoch + 1, (None, None, test_acc))
     print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
                      f'test acc {test_acc:.3f}')
      print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
                      f'on {str(device)}')
```





## 7.ç°ä»£å·ç§¯ç¥ç»ç½‘ç»œ



### VGG

```python
##VGG
import torch
from torch import nn
from d2l import torch as d2l

import os
os.chdir('/home/jupyter-zhangwenkk/data/')


def VGG_block(num_conv,in_channel,out_channel):
    layers = []
    for _ in range(num_conv):
        layers.append(nn.Conv2d(in_channel,out_channel,kernel_size=3,padding=1))
        layers.append(nn.ReLU())
        in_channel = out_channel
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))

    return nn.Sequential(*layers)


conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))

def vgg(con_arch):
    conv_blks = []
    in_channels = 1
    for (num_convs,out_channels) in con_arch:
        conv_blks.append(VGG_block(num_convs,in_channels,out_channels))
        in_channels = out_channels

    return  nn.Sequential(*conv_blks,nn.Flatten(),
                            # å…¨è¿æ¥å±‚éƒ¨åˆ†
                            nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),
                            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
                            nn.Linear(4096, 10))


net = vgg(conv_arch)


X = torch.randn(size=(1, 1, 224, 224))
for blk in net:
    X = blk(X)
    print(blk.__class__.__name__,'output shape:\t',X.shape)


ratio = 4
small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]
net = vgg(small_conv_arch)


lr, num_epochs, batch_size = 0.05, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())



```



### BN





æ‰‹å†™BN

```python
import torch
from torch import nn
from d2l import torch as d2l
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # é€šè¿‡is_grad_enabledæ¥åˆ¤æ–­å½“å‰æ¨¡å¼æ˜¯è®­ç»ƒæ¨¡å¼è¿˜æ˜¯é¢„æµ‹æ¨¡å¼
    if not torch.is_grad_enabled():
        # å¦‚æœæ˜¯åœ¨é¢„æµ‹æ¨¡å¼ä¸‹ï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ç§»åŠ¨å¹³å‡æ‰€å¾—çš„å‡å€¼å’Œæ–¹å·®
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:	
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # ä½¿ç”¨å…¨è¿æ¥å±‚çš„æƒ…å†µï¼Œè®¡ç®—ç‰¹å¾ç»´ä¸Šçš„å‡å€¼å’Œæ–¹å·®
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # ä½¿ç”¨äºŒç»´å·ç§¯å±‚çš„æƒ…å†µï¼Œè®¡ç®—é€šé“ç»´ä¸Šï¼ˆaxis=1ï¼‰çš„å‡å€¼å’Œæ–¹å·®ã€‚
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¿æŒXçš„å½¢çŠ¶ä»¥ä¾¿åé¢å¯ä»¥åšå¹¿æ’­è¿ç®—
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
            # è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œç”¨å½“å‰çš„å‡å€¼å’Œæ–¹å·®åšæ ‡å‡†åŒ–
            X_hat = (X - mean) / torch.sqrt(var + eps)
            # æ›´æ–°ç§»åŠ¨å¹³å‡çš„å‡å€¼å’Œæ–¹å·®
         moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
         moving_var = momentum * moving_var + (1.0 - momentum) * var
      Y = gamma * X_hat + beta
            # ç¼©æ”¾å’Œç§»ä½
      return Y, moving_mean.data, moving_var.dat
```











## ç›®æ ‡æ£€æµ‹



RCNN

å…ˆæå–å›¾ç‰‡çš„ç‰¹å¾  è¾“å‡ºåæ ‡å’Œç‰¹å¾ï¼ŒæŠŠç‰¹å¾é€åˆ°SVMè¾“å‡ºç±»åˆ«















## è¦ç†è§£çš„æ¦‚å¿µ(é¢è¯•)

### æ¢¯åº¦ä¸‹é™

https://zhuanlan.zhihu.com/p/261375491

- ä»€ä¹ˆæ˜¯æ¢¯åº¦ï¼Œæ–¹ç¨‹æ±‚åå¯¼å¾—åˆ°çš„å¼å­å°±æ˜¯æ¢¯åº¦ï¼ŒæŠŠå¯¹åº”çš„xå€¼æ”¾è¿›å»å°±èƒ½å¾—åˆ°xç‚¹çš„æ–œç‡ï¼Œè¿™æ ·å°±å¯ä»¥çŸ¥é“è¦å‡å°‘å€¼æ—¶å‰è¿›çš„æ–¹å‘
- ä½¿ç”¨æ‰€æœ‰æ ·æœ¬æ¥æ›´æ–°æ¢¯åº¦

### éšæœºæ¢¯åº¦ä¸‹é™

SGDçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æ¯æ¬¡è¿­ä»£ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬ï¼ˆæˆ–ä¸€å°æ‰¹æ ·æœ¬ï¼‰æ¥ä¼°è®¡æ¢¯åº¦ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†ã€‚è¿™æ ·åšçš„ä¼˜ç‚¹æ˜¯è®¡ç®—æ•ˆç‡é«˜ï¼Œå°¤å…¶æ˜¯å½“æ•°æ®é›†å¾ˆå¤§æ—¶ã€‚SGDä¹Ÿèƒ½å¤Ÿé€ƒç¦»å±€éƒ¨æœ€å°å€¼ï¼Œå› ä¸ºéšæœºæ€§å¼•å…¥äº†ä¸€å®šçš„å™ªå£°ï¼Œæœ‰åŠ©äºæ¨¡å‹æ¢ç´¢æ›´å¤šçš„å‚æ•°ç©ºé—´ã€‚

è¿™ä¸ªéšæœºçš„æ¢¯åº¦å¦‚ä½•é€‰æ‹©

- ä½¿ç”¨ä¸€ä¸ªæ ·æœ¬æ¥æ›´æ–°æ¢¯åº¦

![image-20240519183139578](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240519183139578.png)

### å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™

https://zhuanlan.zhihu.com/p/72929546

- ä½¿ç”¨ä¸€äº›æ ·æœ¬æ¥æ›´æ–°æ¢¯åº¦

![image-20240529101550727](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240529101550727.png)



### æ‰¹é‡å¤§å°çš„é€‰æ‹©

**æ‰¹é‡å¤§å°ä¸å½±å“éšæœºæ¢¯åº¦çš„æœŸæœ›ï¼Œä½†æ˜¯ä¼šå½±å“éšæœºæ¢¯åº¦çš„æ–¹å·®.** æ‰¹é‡å¤§å°è¶Šå¤§ï¼Œéšæœºæ¢¯åº¦çš„æ–¹å·®è¶Šå°ï¼Œå¼•å…¥çš„å™ªå£°ä¹Ÿè¶Šå°ï¼Œè®­ç»ƒä¹Ÿè¶Šç¨³å®šï¼Œå› æ­¤å¯ä»¥è®¾ç½®è¾ƒå¤§çš„å­¦ä¹ ç‡. è€Œæ‰¹é‡å¤§å°è¾ƒå°æ—¶ï¼Œéœ€è¦è®¾ç½®è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œå¦åˆ™æ¨¡å‹ä¼šä¸æ”¶æ•›. å­¦ä¹ ç‡é€šå¸¸è¦éšç€æ‰¹é‡å¤§å°çš„å¢å¤§è€Œç›¸åº”åœ°å¢å¤§. ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ–¹æ³•æ˜¯çº¿æ€§ç¼©æ”¾è§„åˆ™ï¼ˆLinear Scaling Ruleï¼‰

https://cloud.baidu.com/article/3252849

![image-20240529102922029](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240529102922029.png)

ä¸ºä»€ä¹ˆbatchsizeè¶Šå¤§ï¼Œéšæœºè¯¯å·®è¶Šå°ï¼ˆæ¢¯åº¦çš„æ–¹å·®è¶Šå°ï¼‰ï¼Ÿ

- åœ¨å°æ‰¹æ¬¡ï¼ˆmini-batchï¼‰æ¢¯åº¦ä¸‹é™æ³•ä¸­ï¼Œæ¯æ¬¡è¿­ä»£éƒ½ä¼šè®¡ç®—ä¸€ä¸ªæ‰¹æ¬¡æ ·æœ¬çš„å¹³å‡æ¢¯åº¦ã€‚æ‰¹æ¬¡å¤§å°è¶Šå¤§ï¼Œè¿™ä¸ªå¹³å‡æ¢¯åº¦å°±è¶Šæ¥è¿‘äºæ•´ä¸ªè®­ç»ƒé›†çš„çœŸå®æ¢¯åº¦ã€‚   - è¾ƒå¤§çš„æ‰¹æ¬¡åŒ…å«æ›´å¤šçš„æ ·æœ¬ï¼Œè¿™ä½¿å¾—è®¡ç®—å‡ºçš„æ¢¯åº¦èƒ½å¤Ÿæ›´å¥½åœ°ä»£è¡¨æ•°æ®é›†çš„æ€»ä½“è¶‹åŠ¿ï¼Œä»è€Œå‡å°‘äº†éšæœºè¯¯å·®





### å­¦ä¹ ç‡è¡°å‡ç®—æ³•

å­¦ä¹ ç‡æ˜¯ç¥ç»ç½‘ç»œä¼˜åŒ–æ—¶çš„é‡è¦è¶…å‚æ•°. åœ¨æ¢¯åº¦ä¸‹é™æ³•ä¸­ï¼Œå­¦ä¹ ç‡ ğ›¼ çš„å–å€¼éå¸¸å…³é”®ï¼Œå¦‚æœè¿‡å¤§å°±ä¸ä¼šæ”¶æ•›ï¼Œå¦‚æœè¿‡å°åˆ™æ”¶æ•›é€Ÿåº¦å¤ªæ…¢.å¸¸ç”¨çš„å­¦ä¹ ç‡è°ƒæ•´æ–¹æ³•åŒ…æ‹¬**å­¦ä¹ ç‡è¡°å‡ã€å­¦ä¹ ç‡é¢„çƒ­ã€å‘¨æœŸæ€§å­¦ä¹ ç‡è°ƒæ•´**ä»¥åŠä¸€äº›è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡çš„æ–¹æ³•

å­¦ä¹ ç‡ä¸€å¼€å§‹è¦å¤§ï¼Œä¿è¯æ”¶æ•›é€Ÿåº¦

åæœŸå­¦ä¹ ç‡è¦å˜å°ï¼Œé¿å…æ¥å›éœ‡è¡

å­¦ä¹ ç‡è¡°å‡çš„å¸¸è§æ–¹æ³•

- åˆ†æ®µå¸¸æ•°è¡°å‡
- é€†æ—¶è¡°å‡
- æŒ‡æ•°è¡°å‡
- è‡ªç„¶æŒ‡æ•°è¡°å‡
- ä½™å¼¦è¡°å‡
- å­¦ä¹ ç‡è¡°å‡å¾€å¾€ä¸è¡°å‡ç‡ï¼Œepochï¼Œåˆå§‹å­¦ä¹ ç‡æœ‰å…³



å­¦ä¹ ç‡é¢„çƒ­

- ç”±äºå‚æ•°æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œæ¢¯åº¦å¾€å¾€ä¹Ÿæ¯”è¾ƒå¤§ï¼Œå†åŠ ä¸Šæ¯”è¾ƒå¤§çš„åˆå§‹å­¦ä¹ ç‡ï¼Œä¼šä½¿å¾—è®­ç»ƒä¸ç¨³å®š
- ä¸ºäº†æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æœ€åˆå‡ è½®è¿­ä»£æ—¶ï¼Œé‡‡ç”¨æ¯”è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œç­‰æ¢¯åº¦ä¸‹é™åˆ°ä¸€å®šç¨‹åº¦åå†æ¢å¤åˆ°åˆå§‹çš„å­¦ä¹ ç‡ï¼Œè¿™ç§æ–¹æ³•ç§°ä¸ºå­¦ä¹ ç‡é¢„çƒ­ï¼ˆLearning Rate Warmupï¼‰



å‘¨æœŸæ€§å­¦ä¹ ç‡è°ƒæ•´

- ä¸ºäº†ä½¿å¾—æ¢¯åº¦ä¸‹é™æ³•èƒ½å¤Ÿé€ƒç¦»å±€éƒ¨æœ€å°å€¼æˆ–éç‚¹ï¼Œä¸€ç§ç»éªŒæ€§çš„æ–¹å¼æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘¨æœŸæ€§åœ°å¢å¤§å­¦ä¹ ç‡.
  è™½ç„¶å¢å¤§å­¦ä¹ ç‡å¯èƒ½çŸ­æœŸå†…æœ‰æŸç½‘ç»œçš„æ”¶æ•›ç¨³å®šæ€§ï¼Œä½†ä»é•¿æœŸæ¥çœ‹æœ‰åŠ©äºæ‰¾åˆ°æ›´å¥½çš„å±€éƒ¨æœ€ä¼˜è§£



å¾ªç¯å­¦ä¹ ç‡

- è®©å­¦ä¹ ç‡åœ¨ä¸€ä¸ªåŒºé—´å†…å‘¨æœŸæ€§åœ°å¢å¤§å’Œç¼©å°



**è‡ªé€‚åº”å­¦ä¹ ç‡ç®—æ³•**

AdaGrad

- é€šè¿‡è®¡ç®—æ¢¯åº¦å¹³æ–¹çš„ç´¯è®¡å€¼æ¥è°ƒæ•´å­¦ä¹ ç‡

RMSprop ç®—æ³•

- è®¡ç®—æ¯æ¬¡è¿­ä»£æ¢¯åº¦ gtå¹³æ–¹çš„æŒ‡æ•°è¡°å‡ç§»åŠ¨å¹³å‡ï¼Œ

## å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥

- linearLR 
  - çº¿æ€§å­¦ä¹ ç‡
  - é¢„å…ˆå®šä¹‰èµ·å§‹å­¦ä¹ ç‡ï¼Œç»“æŸå­¦ä¹ ç‡ï¼Œè¿­ä»£æ¬¡æ•°ã€‚ä»èµ·å§‹å­¦ä¹ ç‡å¼€å§‹ç»è¿‡å®šä¹‰çš„è¿­ä»£æ¬¡æ•°è¾¾åˆ°ç»“æŸå­¦ä¹ ç‡

![image-20240508145447541](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240508145447541.png)

- CosineAnnealingLR
  - ä½™å¼¦é€€ç«å­¦ä¹ ç‡
  - å®šä¹‰åˆå§‹å­¦ä¹ ç‡ï¼Œç»“æŸå­¦ä¹ ç‡ï¼Œå®šä¹‰å­¦ä¹ ç‡å‘¨æœŸã€‚åœ¨ä¸€ä¸ªåæœŸå†…ä¸‹é™ï¼Œç„¶åä¸€ä¸ªå‘¨æœŸå†…ä¸Šå‡

![image-20240508145812784](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240508145812784.png)

- ç»„åˆä¸¤ä¸ªå­¦ä¹ ç‡ï¼Œè¾¾åˆ°å­¦ä¹ ç‡é¢„çƒ­çš„ç›®çš„

![image-20240508150240302](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240508150240302.png)



**åŠ¨é‡ç®—æ³•**

Adam

- è®¡ç®—æ¢¯åº¦å¹³æ–¹çš„æŒ‡æ•°åŠ æƒå¹³å‡ï¼Œè®¡ç®—æ¢¯åº¦ çš„æŒ‡æ•°åŠ æƒå¹³å‡

**æ¢¯åº¦æˆªæ–­**

- æ¢¯åº¦çš„æ¨¡å°äºæˆ–å¤§äºè¿™ä¸ªåŒºé—´æ—¶å°±è¿›è¡Œæˆªæ–­.é¿å…æ¢¯åº¦æ¶ˆå¤±(çˆ†ç‚¸)









### å‚æ•°åˆå§‹åŒ–

åœ¨ç¬¬ä¸€éå‰å‘è®¡ç®—æ—¶ï¼Œå¦‚æœå‚æ•°éƒ½ä¸º 0ï¼Œæ‰€æœ‰çš„éšå±‚ç¥ç»å…ƒçš„æ¿€æ´»å€¼éƒ½ç›¸åŒ. è¿™æ ·ä¼šå¯¼è‡´æ·±å±‚ç¥ç»å…ƒæ²¡æœ‰åŒºåˆ†æ€§. è¿™ç§ç°è±¡ä¹Ÿç§°ä¸º**å¯¹ç§°æƒé‡**ç°è±¡.



æˆ‘ä»¬è¦é¿å…è¿™ä¸ªç°è±¡ï¼Œå¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œéšæœºåˆå§‹åŒ–

- å¦‚æœåˆå§‹åŒ–å‚æ•°å¤ªå°ï¼Œä¼šå¯¼è‡´ç½‘ç»œæ¢¯åº¦æ¶ˆå¤±ï¼Œå¦‚æœè¿‡å¤§ä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ï¼ˆæ ¹æ®ä½¿ç”¨äº†å“ªä¸€ä¸ªæ¿€æ´»å‡½æ•°æ¥å®šä¹‰ï¼Œä¸åŒçš„æ¿€æ´»å‡½æ•°é€ æˆçš„åæœå¯èƒ½ä¸ä¸€æ ·ï¼‰



å¸¸ç”¨çš„åˆå§‹åŒ–æ–¹æ³•

- é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–
  - å‚æ•°ä»ä¸€ä¸ªå›ºå®šå‡å€¼ï¼ˆæ¯”å¦‚ 0ï¼‰å’Œå›ºå®šæ–¹å·®ï¼ˆæ¯”å¦‚ 0.01ï¼‰çš„é«˜æ–¯åˆ†å¸ƒè¿›è¡Œéšæœºåˆå§‹åŒ–.
- å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–
  - åœ¨ä¸€ä¸ªç»™å®šçš„åŒºé—´[-r,r]å†…é‡‡ç”¨å‡åŒ€åˆ†å¸ƒæ¥åˆå§‹åŒ–å‚æ•°. è¶…å‚æ•° r çš„è®¾ç½®å¯ä»¥æŒ‰ç¥ç»å…ƒçš„è¿æ¥æ•°é‡è¿›è¡Œè‡ªé€‚åº”çš„è°ƒæ•´
- Xavier åˆå§‹åŒ–
  - æ ¹æ®æ¯å±‚çš„ç¥ç»å…ƒæ•°é‡æ¥è‡ªåŠ¨è®¡ç®—åˆå§‹åŒ–å‚æ•°çš„æ–¹å·®.





### æ•°æ®é¢„å¤„ç†



- æ ‡å‡†å½’ä¸€åŒ–å¤„ç†
- ç™½åŒ–å¤„ç†
  - ç”¨æ¥é™ä½è¾“å…¥æ•°æ®ç‰¹å¾ä¹‹é—´çš„å†—ä½™æ€§. è¾“å…¥æ•°æ®ç»è¿‡ç™½åŒ–å¤„ç†åï¼Œç‰¹å¾ä¹‹é—´ç›¸å…³æ€§è¾ƒä½ï¼Œå¹¶ä¸”æ‰€æœ‰ç‰¹å¾å…·æœ‰ç›¸åŒçš„æ–¹å·®.
  - ç™½åŒ–çš„ä¸€ä¸ªä¸»è¦å®ç°æ–¹å¼æ˜¯ä½¿ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPrincipal Component Analy-sisï¼ŒPCAï¼‰æ–¹æ³•å»é™¤æ‰å„ä¸ªæˆåˆ†ä¹‹é—´çš„ç›¸å…³æ€§

![image-20240529150815252](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240529150815252.png)

### è¶…å‚æ•°ä¼˜åŒ–

- ç½‘æ ¼æœç´¢
  - é€šè¿‡å°è¯•æ‰€æœ‰è¶…å‚æ•°çš„ç»„åˆæ¥å¯»å€åˆé€‚ä¸€ç»„è¶…å‚æ•°é…ç½®çš„æ–¹æ³•
- éšæœºæœç´¢
  - ä¸€ç§åœ¨å®è·µä¸­æ¯”è¾ƒæœ‰æ•ˆçš„æ”¹è¿›æ–¹æ³•æ˜¯å¯¹è¶…å‚æ•°è¿›è¡Œéšæœºç»„åˆï¼Œç„¶åé€‰å–ä¸€ä¸ªæ€§èƒ½æœ€å¥½çš„é…ç½®



BNå±‚çš„ä½œç”¨

![image-20241220110621380](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20241220110621380.png)









### softmaxå›å½’

![image-20240422102220406](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240422102220406.png)

**softmaxè¦é…åˆäº¤å‰ç†µæŸå¤±å‡½æ•°åŒæ—¶å®ç°**

- è¿™é‡Œéœ€è¦æ³¨æ„ä¸€ä¸‹ï¼Œå½“ä½¿ç”¨Softmaxå‡½æ•°ä½œä¸ºè¾“å‡ºèŠ‚ç‚¹çš„æ¿€æ´»å‡½æ•°çš„æ—¶å€™ï¼Œä¸€èˆ¬ä½¿ç”¨äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°ã€‚ç”±äºSoftmaxå‡½æ•°çš„æ•°å€¼è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œå¾ˆå®¹æ˜“å› ä¸ºè¾“å‡ºèŠ‚ç‚¹çš„è¾“å‡ºå€¼æ¯”è¾ƒå¤§è€Œå‘ç”Ÿæ•°å€¼æº¢å‡ºçš„ç°è±¡ï¼Œåœ¨è®¡ç®—äº¤å‰ç†µçš„æ—¶å€™ä¹Ÿå¯èƒ½ä¼šå‡ºç°æ•°å€¼æº¢å‡ºçš„é—®é¢˜ã€‚







### æ¿€æ´»å‡½æ•°



sigmoid

- sigmoidé€šå¸¸ç§°ä¸ºæŒ¤å‹å‡½æ•°ï¼ˆsquashing functionï¼‰ï¼šå®ƒå°†èŒƒå›´ï¼ˆâ€inf, infï¼‰ä¸­çš„ä»»æ„è¾“å…¥å‹ç¼©åˆ°åŒºé—´ï¼ˆ0, 1ï¼‰ä¸­çš„æŸä¸ªå€¼
- $sigmoid(x) = \frac{1}{1+e^{-x}}$
- ![image-20240407152025211](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240407152025211.png)
- ![image-20240407170440187](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240407170440187.png)

sigmoidå‡½æ•°çš„ç¼ºç‚¹ï¼Œå–å€¼è¢«æ˜ å°„åˆ°0-1èŒƒå›´å†…ï¼Œæ‰€ä»¥ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜

- ReLU
  - ReLU(x) = max(x, 0)
  - ReLUæ¿€æ´»å‡½æ•°ç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œè¿™æ ·å¯ä»¥åŠ é€Ÿæ”¶æ•›

![image-20240407150503304](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240407150503304.png)

ReLUçš„å¥½å¤„

https://zhuanlan.zhihu.com/p/390655512

- èƒ½å¤Ÿé¿å…åå‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ¶ˆå¤±ã€å±è”½è´Ÿå€¼ã€é˜²æ­¢æ¢¯åº¦é¥±å’Œ

ReLUçš„ç¼ºç‚¹

- å½“å‚æ•°å–å€¼ä¸ºè´Ÿæ•°æ—¶ï¼ŒReLUæ¿€æ´»å‡½æ•°ä¼šè¾“å‡º0ï¼Œ

- ![image-20240521162531230](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240521162531230.png)

![image-20240528152751946](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240528152751946.png)









- LeaKyReLU
  - åœ¨å°äº0çš„éƒ¨åˆ†ä¸åƒReLUæ˜¯0ï¼Œè€Œæ˜¯ä¸€ä¸ªè´Ÿæ•°çš„éšæœºæ¢¯åº¦ï¼Œaçš„å€¼ä¸€èˆ¬ä¸º0.01

![image-20240422160856843](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240422160856843.png)







- tanhå‡½æ•°
  - $tanh(x) = \frac{1-e^{-2x}}{1+e^{-2x}}$ 
  - ![image-20240407154420481](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240407154420481.png)

![image-20240411152150367](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240411152150367.png)

**SiLu(swish)æ¿€æ´»å‡½æ•°**

f(x) = x * sigmoid(x)  (yolov5)

å®ƒæ—¢æœ‰ ReLUï¼ˆRectified Linear Unitï¼‰æ¿€æ´»å‡½æ•°çš„ä¸€äº›ä¼˜ç‚¹ï¼ˆä¾‹å¦‚ï¼Œèƒ½å¤Ÿç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼‰ï¼Œåˆèƒ½è§£å†³ ReLU å‡½æ•°çš„ä¸€äº›ç¼ºç‚¹ï¼ˆä¾‹å¦‚ï¼ŒReLU å‡½æ•°ä¸æ˜¯é›¶ä¸­å¿ƒçš„ï¼Œä¸”åœ¨è´Ÿæ•°éƒ¨åˆ†çš„æ¢¯åº¦ä¸ºé›¶ï¼‰

![image-20240418183949125](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240418183949125.png)





### æ¿€æ´»å‡½æ•°çš„é€‰æ‹©

å¦‚æœæ­å»ºçš„ç¥ç»ç½‘ç»œå±‚æ•°ä¸å¤šï¼Œé€‰æ‹©sigmoidã€tanhã€reluã€softmaxéƒ½å¯ä»¥ï¼›è€Œå¦‚æœæ­å»ºçš„ç½‘ç»œå±‚æ¬¡è¾ƒå¤šï¼Œé‚£å°±éœ€è¦å°å¿ƒï¼Œ**é€‰æ‹©ä¸å½“å°±å¯å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**ã€‚**æ­¤æ—¶ä¸€èˆ¬ä¸å®œé€‰æ‹©sigmoidã€tanhæ¿€æ´»å‡½æ•°ï¼Œå› å®ƒä»¬çš„å¯¼æ•°éƒ½å°äº1**ï¼Œæ‰€ä»¥ï¼Œæ­å»ºæ¯”è¾ƒæ·±çš„ç¥ç»ç½‘ç»œæ—¶ï¼Œä¸€èˆ¬ä½¿ç”¨reluæ¿€æ´»å‡½æ•°





### æŸå¤±å‡½æ•°çš„é€‰æ‹©

ä»€ä¹ˆæ˜¯æŸå¤±å‡½æ•°ï¼Ÿ

æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰åœ¨æœºå™¨å­¦ä¹ ä¸­éå¸¸é‡è¦ï¼Œå› ä¸ºè®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹å®é™…å°±æ˜¯ä¼˜åŒ–æŸå¤±å‡½æ•°çš„è¿‡ç¨‹ã€‚æŸå¤±å‡½æ•°ç”¨æ¥è¡¡é‡æ¨¡å‹çš„å¥½åï¼ŒæŸå¤±å‡½æ•°è¶Šå°è¯´æ˜æ¨¡å‹å’Œå‚æ•°è¶Šç¬¦åˆè®­ç»ƒæ ·æœ¬ã€‚



å¸¸ç”¨çš„æŸå¤±å‡½æ•°æœ‰ä¸¤ç§

- äº¤å‰ç†µ(Cross Entropy) softmaxæ¿€æ´»å‡½æ•°
- https://www.zhihu.com/tardis/zm/art/35709485?source_id=1005
  - åˆ†ç±»é—®é¢˜ (äº¤å‰ç†µååº”çš„ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„è·ç¦»)
    - å¤šåˆ†ç±»
      - softmaxæ¿€æ´»å‡½æ•°+äº¤å‰ç†µæŸå¤±å‡½æ•°
    - äºŒåˆ†ç±» ï¼ˆé€»è¾‘å›å½’æŸå¤±ï¼ˆLogistic Lossï¼‰ï¼‰

![image-20240501153923650](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240501153923650.png)

**ä¸ºä»€ä¹ˆäº¤å‰ç†µå¯ä»¥è®¡ç®—ç±»åˆ«çš„æŸå¤±**

https://zhuanlan.zhihu.com/p/124309304

- **äº¤å‰ç†µï¼ˆCross Entropyï¼‰ç”¨äºè¡¡é‡ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸å¦ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚äº¤å‰ç†µæ˜¯æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸­å¸¸å¸¸ç”¨åˆ°çš„ä¸€ä¸ªæ¦‚å¿µã€‚åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æœ‰ä¸€ä¸ªçœŸå®çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯ä¸“å®¶æˆ–è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒï¼‰ï¼Œä»¥åŠä¸€ä¸ªæ¨¡å‹ç”Ÿæˆçš„æ¦‚ç‡åˆ†å¸ƒï¼Œäº¤å‰ç†µå¯ä»¥è¡¡é‡è¿™ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚æ¨¡å‹è®­ç»ƒæ—¶ï¼Œé€šè¿‡æœ€å°åŒ–äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿æ¨¡å‹é¢„æµ‹å€¼çš„æ¦‚ç‡åˆ†å¸ƒé€æ­¥æ¥è¿‘çœŸå®çš„æ¦‚ç‡åˆ†å¸ƒã€‚**

- ç¥ç»ç½‘ç»œçš„ç±»åˆ«è¾“å‡ºå¾€å¾€æ˜¯è¾“å‡ºç±»åˆ«çš„æ¦‚ç‡å€¼ï¼Œè¿™æ—¶å€™è¦è®¡ç®—è¾“å‡ºæ¦‚ç‡å€¼ä¸æ ‡ç­¾æ¦‚ç‡ä¹‹é—´çš„è¯¯å·®ã€‚è¿™ä¸ªè¦è¿½æº¯åˆ°ä¿¡æ¯è®ºï¼Œé¦™æµ“ç†µã€‚å½“æ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒä¸ºPï¼Œæ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡ä¸ºQï¼Œæˆ‘ä»¬è¦å‡å°Pï¼ŒQä¹‹é—´çš„è¯¯å·®ï¼Œå°±è¦å¯¹PQçš„äº¤å‰ç†µåšæœ€å°åŒ–å¤„ç†ï¼Œè¿™æ—¶å€™å°±è¦æœ€å°åŒ–PQçš„äº¤å‰ç†µï¼Œè®©ä»–ä»¬çš„æ¦‚ç‡åˆ†å¸ƒæ›´æ¥è¿‘

![image-20240505212319942](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240505212319942.png)

















- å‡æ–¹è¯¯å·®ï¼ˆMean squared errorï¼ŒMSE)
- https://blog.csdn.net/Xiaobai_rabbit0/article/details/111032136
  - å›å½’é—®é¢˜ï¼ˆå›å½’é—®é¢˜é¢„æµ‹çš„ä¸æ˜¯ç±»åˆ«ï¼Œè€Œæ˜¯ä¸€ä¸ªä»»æ„å®æ•°ï¼‰



![image-20240501154023940](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240501154023940.png)

### ä¼˜åŒ–å™¨



![image-20240411153814708](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240411153814708.png)



- ä¼ ç»Ÿæ¢¯åº¦ä¸‹é™



- åŠ¨é‡ç®—æ³• + æ¢¯åº¦ä¸‹é™
  - NAG![image-20240411154623342](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240411154623342.png)

- å­¦ä¹ ç‡ç®—æ³•

  - AdaGradï¼ˆè‡ªåŠ¨æ›´æ–°å­¦ä¹ ç‡ï¼‰
    - ä¸ºäº†æ›´å¥½åœ°é©¾é©­è¿™ä¸ªè¶…å‚æ•°ï¼Œäººä»¬æƒ³å‡ºæ¥å¤šç§è‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•ï¼Œä½¿ç”¨è‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•ï¼Œå­¦ä¹ ç‡ä¸
      å†æ˜¯ä¸€ä¸ªå›ºå®šä¸å˜å€¼ï¼Œå®ƒä¼šæ ¹æ®ä¸åŒæƒ…å†µè‡ªåŠ¨è°ƒæ•´æ¥é€‚åº”ç›¸åº”çš„æƒ…å†µ

  - RMSPropç®—æ³•
    - é’ˆå¯¹æ¢¯åº¦å¹³æ–¹å’Œç´¯è®¡è¶Šæ¥è¶Šå¤§çš„é—®é¢˜ï¼ŒRMSPropæŒ‡æ•°åŠ æƒçš„ç§»åŠ¨å¹³å‡ä»£æ›¿æ¢¯åº¦å¹³æ–¹å’Œ
  - Adam
    - Adamï¼ˆAdaptive Moment Estimationï¼‰æœ¬è´¨ä¸Šæ˜¯å¸¦æœ‰åŠ¨é‡é¡¹çš„RMSpropï¼Œå®ƒåˆ©ç”¨æ¢¯åº¦
      çš„ä¸€é˜¶çŸ©ä¼°è®¡å’ŒäºŒé˜¶çŸ©ä¼°è®¡åŠ¨æ€è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡
    - Adam é€šè¿‡è®¡ç®—æ¢¯åº¦çš„ä¸€é˜¶çŸ©ä¼°è®¡å’ŒäºŒé˜¶çŸ©ä¼°è®¡çš„ç§»åŠ¨å¹³å‡å€¼ï¼Œè°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡



ç®—æ³•çš„ç»¼åˆä½¿ç”¨

- **æœ‰æ—¶å¯ä»¥è€ƒè™‘ç»¼åˆä½¿ç”¨è¿™äº›ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚é‡‡ç”¨å…ˆä½¿ç”¨Adamï¼Œç„¶åä½¿ç”¨SGDçš„ä¼˜åŒ–æ–¹**
  **æ³•ï¼Œè¿™ä¸ªæƒ³æ³•ï¼Œå®é™…ä¸Šæ˜¯ç”±äºåœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µSGDå¯¹å‚æ•°è°ƒæ•´å’Œåˆå§‹åŒ–éå¸¸æ•æ„Ÿã€‚å› **
  **æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å…ˆä½¿ç”¨Adamä¼˜åŒ–ç®—æ³•æ¥è¿›è¡Œè®­ç»ƒï¼Œè¿™å°†å¤§å¤§åœ°èŠ‚çœè®­ç»ƒæ—¶é—´ï¼Œä¸”ä¸å¿…**
  **æ‹…å¿ƒåˆå§‹åŒ–å’Œå‚æ•°è°ƒæ•´ï¼Œä¸€æ—¦ç”¨Adamè®­ç»ƒè·å¾—è¾ƒå¥½çš„å‚æ•°åï¼Œå°±å¯ä»¥åˆ‡æ¢åˆ°SGD+åŠ¨é‡**
  **ä¼˜åŒ–ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½**





adamW

- ä¼˜åŒ–å™¨æ˜¯ç”¨æ¥ä¼˜åŒ–æ¢¯åº¦çš„ï¼Œé€šè¿‡è·ç¦»èŒƒæ•°æ¥ä¼˜åŒ–ä¸‹ä¸€æ¬¡ä¸‹é™çš„æ¢¯åº¦å€¼

![image-20240508151557549](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240508151557549.png)

















æ¬ æ‹Ÿåˆï¼ˆæ¨¡å‹å¤ªç®€å•ï¼Œæ— æ³•å¾ˆå¥½å­¦ä¹ åˆ°æ•°æ®é›†ä¸­çš„ç‰¹å¾ï¼Œï¼‰

è¿‡æ‹Ÿåˆï¼ˆæ¨¡å‹å¤ªå¤æ‚ï¼Œå­¦ä¹ æ•°æ®é›†ä¸­çš„ç‰¹å¾å­¦è¿‡å¤´äº†ï¼Œæ•°æ®é›†å¤ªç®€å•ï¼‰

![image-20240407160037091](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240407160037091.png)

- æ•°æ®é›†å¤ªå°å®¹æ˜“è¿‡æ‹Ÿåˆ
- æ•°æ®é›†å¤ªå¤§å®¹æ˜“æ¬ æ‹Ÿåˆ(æ¨¡å‹æ— æ³•å¾ˆå¥½å­¦ä¹ æ•°æ®é›†ä¸­çš„è§„å¾‹)











### è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜

- æƒé‡è¡°å‡
  - æœ€å¸¸ç”¨æ–¹æ³•æ˜¯å°†æƒé‡å‘é‡çš„èŒƒæ•°ä½œä¸ºæƒ©ç½šé¡¹åŠ åˆ°æœ€å°åŒ–æŸå¤±çš„é—®é¢˜ä¸­ã€‚**æƒé‡è¡°å‡é€šè¿‡å¯¹æ¨¡å‹çš„æƒé‡å‚æ•°è¿›è¡Œæƒ©ç½šï¼Œä½¿å¾—æ¨¡å‹å€¾å‘äºå­¦ä¹ ç®€å•çš„æƒé‡åˆ†å¸ƒï¼Œè€Œä¸æ˜¯å¤æ‚çš„åˆ†å¸ƒï¼Œä»è€Œè®©æ¨¡å‹å˜å¾—ç®€å•ã€‚**
- dropoutï¼ˆæš‚é€€æ³•ï¼‰ï¼ˆä¸¢å¼ƒæ³•ï¼‰
  - f(x) = wx + b  å½“w=0æ—¶ï¼Œç›¸å½“äºè¿™ä¸ªç¥ç»å…ƒä¸ä½œä¸ºè¾“å…¥äº†ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¸¢å¼ƒä¸€äº›ç¥ç»å…ƒï¼Œæ‰€ä»¥æ¨¡å‹æœ‰å˜ç®€å•äº†

- æå‰åœæ­¢
  - å½“éªŒè¯é›†ä¸Šçš„é”™è¯¯ç‡ä¸å†ä¸‹é™ï¼Œå°±åœæ­¢è¿­ä»£

### æ¢¯åº¦æ¶ˆå¤±

- å‚æ•°æ›´æ–°è¿‡å°ï¼Œåœ¨æ¯æ¬¡æ›´æ–°æ—¶æƒé‡å‡ ä¹ä¸ä¼šç§»åŠ¨ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•å­¦ä¹ 
- ç”¨äº†ä¸åˆé€‚çš„æ¿€æ´»å‡½æ•°

#### æ¢¯åº¦çˆ†ç‚¸

- å‚æ•°æ›´æ–°è¿‡å¤§ï¼Œç ´åäº†æ¨¡å‹çš„ç¨³å®šæ”¶æ•›ï¼›
- æ¢¯åº¦çš„åˆå§‹å€¼å¤ªå¤§ï¼Œé™ä½åˆå§‹å€¼ï¼Œè®¾ç½®æ¢¯åº¦é˜ˆå€¼ï¼Œå¼ºåˆ¶åœ¨è¿™ä¸ªèŒƒå›´å†…
- é€šè¿‡æƒé‡æ­£åˆ™é¡¹æƒ©ç½šæ¥é™åˆ¶æƒé‡å¤§å°
- batch normalization **å¯¹æ¯ä¸€å±‚çš„è¾“å‡ºè§„èŒƒä¸ºå‡å€¼å’Œæ–¹å·®ä¸€è‡´çš„æ–¹æ³•**ï¼Œæ¶ˆé™¤æƒé‡å‚æ•°æ”¾å¤§ç¼©å°å¸¦æ¥çš„å½±å“ï¼Œä»è€Œè§£å†³æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸ï¼ˆBNå°†è¾“å‡ºä»é¥±å’ŒåŒºæ‹‰åˆ°éé¥±å’ŒåŒºï¼‰



### æ¨¡å‹è½»é‡åŒ–

- DWå·ç§¯çš„ä½¿ç”¨

- ç‚¹å·ç§¯ä»£æ›¿å…¨è¿æ¥å±‚



### å·ç§¯çš„ç‰¹æ€§

- å¹³ç§»ä¸å˜æ€§
  - å·ç§¯è¿‡ç¨‹ä¸­å·ç§¯æ ¸æ˜¯å¹³ç§»çš„ï¼Œä½†æ˜¯è¾“å…¥ç‰¹å¾çš„ä½ç½®å¯¹åº”ç€è¾“å‡ºç‰¹å¾çš„ä½ç½®
- å±€éƒ¨æ€§
  - å·ç§¯æ ¸ æ¢ç´¢è¾“å…¥å›¾åƒä¸­çš„å±€éƒ¨åŒºåŸŸï¼Œä¸åœ¨æ„å›¾åƒä¸­ç›¸éš”è¾ƒè¿œçš„åŒºåŸŸå…³ç³»



è¾“å‡ºHWçš„è®¡ç®—å…¬å¼

![image-20240410094800541](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240410094800541.png)

æ±‡èšå±‚

- æ± åŒ–å±‚



### æ‰¹é‡å½’ä¸€åŒ–(batch normalization)

è®©æ•°æ®å˜æˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1çš„åˆ†å¸ƒï¼Œè®©æ•°æ®å˜æˆç‹¬ç«‹åŒåˆ†å¸ƒ

è¿™æ ·åšæœ‰ä»€ä¹ˆå¥½å¤„

- è®©æ¨¡å‹æ›´å¥½çš„è®­ç»ƒ
  - è®©ç‰¹å¾çš„æ•°å€¼æ˜ å°„åˆ°ä¸€ä¸ªåŒºé—´åˆ†å¸ƒã€‚å¦‚æœä¸€ä¸ªç‰¹å¾çš„åˆ†å¸ƒè¶‹å‘äºä¸¤è¾¹ï¼Œä½¿ç”¨sigmoidæ¿€æ´»å‡½æ•°æ¿€æ´»æ—¶ï¼Œå¯èƒ½è¦ä¹ˆæ˜¯0 è¦ä¹ˆæ˜¯1
- å‡å°‘è®­ç»ƒå’Œæµ‹è¯•çš„è¯¯å·® ï¼ˆè®©è®­ç»ƒå’Œæµ‹è¯•æ—¶ï¼Œæ•°æ®çš„åˆ†å¸ƒä¸€æ ·ï¼‰
  - è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®çš„åˆ†å¸ƒæ˜¯ç›¸åŒçš„ï¼Œè¿™å‡å°‘äº†æ¨¡å‹çš„åå·®é£é™©ã€‚è¿™æ ·ï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„è¡¨ç°å’Œåœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°æ›´åŠ ä¸€è‡´ã€‚





### å‡¸å‡½æ•°ä¸éå‡¸ä¼˜åŒ–

![image-20240410161028636](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240410161028636.png)



### ç‰¹å¾è¡¨ç¤ºå½¢å¼

å±€éƒ¨è¡¨ç¤º

åˆ†å¸ƒå¼è¡¨ç¤º





### åˆ†æä¸ºä»€ä¹ˆå¹³æ–¹æŸå¤±å‡½æ•°ä¸é€‚ç”¨äºåˆ†ç±»é—®é¢˜

å¹³æ–¹æŸå¤±å‡½æ•°æ˜¯è®¡ç®—ä¸¤ä¸ªè¿ç»­å€¼ä¹‹é—´çš„è·ç¦»ï¼Œè€Œåˆ†ç±»é—®é¢˜è¾“å‡ºçš„æ˜¯ä¸€ä¸ªç¦»æ•£å€¼ï¼ˆæ¦‚ç‡ï¼‰ï¼Œæ ‡ç­¾ä¹Ÿæ˜¯ä¸€ä¸ªç¦»æ•£å€¼ï¼Œæ²¡æœ‰è¿ç»­çš„æ¦‚å¿µã€‚å¦‚æœä½¿ç”¨å¹³æ–¹æŸå¤±å‡½æ•°è®¡ç®—é¢„æµ‹å€¼ä¸æ ‡ç­¾ä¹‹é—´çš„è·ç¦»æ²¡æœ‰å®é™…æ„ä¹‰ï¼Œé¢„æµ‹å€¼å’Œæ ‡ç­¾ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å¹³æ–¹å·®è¿™ä¸ªå€¼ä¸èƒ½ååº”åˆ†ç±»è¿™ä¸ªé—®é¢˜çš„ä¼˜åŒ–ç¨‹åº¦ã€‚



### å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰

å¤šå±‚æ„ŸçŸ¥æœºMLPå±‚å¾€å¾€é€šè¿‡Linearé™ä¸º -ã€‹æ¿€æ´»å‡½æ•°-ã€‹Linearå‡ç»´

ä¸ºä»€ä¹ˆè¦é™ç»´

- åœ¨åŸå§‹çš„é«˜ç»´ç©ºé—´ä¸­ï¼ŒåŒ…å«å†—ä½™ä¿¡æ¯å’Œå™ªå£°ä¿¡æ¯ï¼Œä¼šåœ¨å®é™…åº”ç”¨ä¸­å¼•å…¥è¯¯å·®ï¼Œå½±å“å‡†ç¡®ç‡ï¼›
- è€Œé™ç»´å¯ä»¥æå–æ•°æ®å†…éƒ¨çš„æœ¬è´¨ç»“æ„ï¼Œå‡å°‘å†—ä½™ä¿¡æ¯å’Œå™ªå£°ä¿¡æ¯é€ æˆçš„è¯¯å·®ï¼Œæé«˜åº”ç”¨ä¸­çš„ç²¾åº¦ã€‚

![image-20240521170640598](https://zhangwenkk333.oss-cn-beijing.aliyuncs.com/image/image-20240521170640598.png)



ä¸ºä»€ä¹ˆè¦æ¿€æ´»å‡½æ•°

- ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼Œæ¯ä¸€å±‚è¾“å‡ºéƒ½æ˜¯ä¸Šå±‚è¾“å…¥çš„çº¿æ€§å‡½æ•°ï¼Œæ— è®ºç¥ç»ç½‘ç»œæœ‰å¤šå°‘å±‚ï¼Œè¾“å‡ºéƒ½æ˜¯è¾“å…¥çš„çº¿æ€§ç»„åˆã€‚
- ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼Œèƒ½å¤Ÿç»™ç¥ç»å…ƒå¼•å…¥éçº¿æ€§å› ç´ ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥ä»»æ„é€¼è¿‘ä»»ä½•éçº¿æ€§å‡½æ•°ï¼Œè¿™æ ·ç¥ç»ç½‘ç»œå°±å¯ä»¥åˆ©ç”¨åˆ°æ›´å¤šçš„éçº¿æ€§æ¨¡å‹ä¸­ã€‚













### å¸¸ç”¨åº“

- torchvision.transforms
- torchvision.datasets
- torch.utils.data.DataLoader
- torch.nn as nn (ç‰¹å¾æå–æ¨¡å— æŸå¤±å‡½æ•°)
- torch.nn.functional as F ï¼ˆæ¿€æ´»å‡½æ•°ï¼‰
- torch.optim as optim

```python
import torch
import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
download=False, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
download=False, transform=transform)

testloader = torch.utils.data.DataLoader(testset, batch_size=4,
shuffle=False, num_workers=2)

dataiter = iter(trainloader)
images, labels = dataiter.next()



import torch.nn as nn
import torch.nn.functional as F
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
class CNNNet(nn.Module):
    def __init__(self):
        super(CNNNet,self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=1)
        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)
        self.conv2 = nn.Conv2d(in_channels=16,out_channels=36,kernel_size=3,stride=1)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(1296,128)
        self.fc2 = nn.Linear(128,10)
    def forward(self,x):
        x=self.pool1(F.relu(self.conv1(x)))
        x=self.pool2(F.relu(self.conv2(x)))
         #print(x.shape)
        x=x.view(-1,36*6*6)
        x=F.relu(self.fc2(F.relu(self.fc1(x))))
        return x
net = CNNNet()
net=net.to(device)

import torch.optim as optim
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)


# è®­ç»ƒæ¨¡å‹
for epoch in range(10):
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # è·å–è®­ç»ƒæ•°æ®
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        # æƒé‡å‚æ•°æ¢¯åº¦æ¸…é›¶
        optimizer.zero_grad()
        # æ­£å‘åŠåå‘ä¼ æ’­
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        # æ˜¾ç¤ºæŸå¤±å€¼
        running_loss += loss.item()
        if i % 2000 == 1999: # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0
            print('Finished Training')
            
            
# æµ‹è¯•æ¨¡å‹
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        images, labels = images.to(device), labels.to(device)
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        print('Accuracy of the network on the 10000 test images: %d %%' % (
            100 * correct / total))

```

